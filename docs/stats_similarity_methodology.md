# Statistical Methods & Similarity Scoring for Physician Record Linkage

## Overview
This document covers the statistical foundation of the record linkage pipeline: why each similarity metric was chosen, how features were engineered, what the five-path rule classifier does, and how ground truth evaluation works. It serves as the methodological companion to the ML Classification doc — explaining the *why* behind every scoring decision.

**Context**: 4,683 tier-2 Open Payments records (no reliable NPI) must be fuzzy-matched against 1,175,281 Medicare providers using only name, address, and phonetic features across 492,427 candidate pairs generated by Phase 3 blocking.

---

## Why Similarity Scoring?

### The Core Problem
Tier-2 OP records lack a deterministic key (NPI) for joining to Medicare. The same physician may appear as:

| Field | Open Payments | Medicare |
|-------|--------------|---------|
| First Name | `MICHAEL` | `MIKE` |
| Last Name | `O'BRIEN` | `OBRIEN` |
| Street | `123 Main St Ste 200` | `123 MAIN STREET` |
| City | `NEW YORK` | `NEW YORK` |
| ZIP | `10001` | `10001` |

Exact string matching would fail on every field except city and ZIP. Similarity scoring converts "how close are these strings?" into continuous [0, 1] scores that capture partial matches, abbreviations, typos, and formatting differences.

### Why Multiple Metrics?
No single similarity metric handles all name variation types. Each metric captures a different dimension of "closeness":

| Variation Type | Example | Best Metric |
|---------------|---------|-------------|
| Typos / transpositions | `MICHEAL` → `MICHAEL` | Jaro-Winkler (prefix-weighted) |
| Abbreviations | `ROBERT` → `ROB` | Normalized Levenshtein (edit distance) |
| Phonetic equivalence | `SMITH` / `SMYTH` | Soundex, Metaphone |
| Hyphenation / order | `MARY JANE SMITH` → `SMITH, MARY-JANE` | Full name JW (concatenated) |
| Length differences | `KIM` vs `KIMBERLY` | Name length ratio |

Using a **feature vector** of multiple metrics lets the classifier combine weak signals — a pair might score 0.78 on `first_jw` (below any single threshold) but 1.0 on `last_lev` + 1.0 on `zip5_match`, collectively forming a strong match signal.

---

## Similarity Metrics: Definitions & Rationale

### Jaro-Winkler Similarity (`first_jw`, `last_jw`, `street_jw`, `full_name_jw`)

**What it measures**: Character-level similarity with a bonus for matching prefixes.

**Formula**: JW(s1, s2) = Jaro(s1, s2) + l * p * (1 - Jaro(s1, s2))

Where:
- Jaro score accounts for matching characters and transpositions
- l = length of common prefix (up to 4 characters)
- p = scaling factor (standard 0.1)

**Why chosen**: Medical names often share prefixes (`JOHN` vs `JONATHAN`, `CHRIS` vs `CHRISTOPHER`). The prefix bonus rewards these partial matches more than generic edit distance. JW is also robust to transpositions (`MICHEAL` → `MICHAEL` scores ~0.98).

**Range**: [0, 1] where 1.0 = identical strings

**Applied to**: First names, last names, street addresses, and concatenated full names (`first + " " + last`).

### Normalized Levenshtein Similarity (`first_lev`, `last_lev`)

**What it measures**: 1 - (minimum edit operations / max string length). Counts insertions, deletions, and substitutions needed to transform one string into another.

**Why chosen**: Captures a fundamentally different similarity dimension than JW. Levenshtein penalizes length differences heavily (`KIM` vs `KIMBERLY` = 0.375), while JW is more forgiving (~0.82). Using both creates a richer feature space — pairs that score high on both metrics are almost certainly matches; pairs that score high on one but low on the other are interesting edge cases.

**Range**: [0, 1] where 1.0 = identical strings

**Applied to**: First names and last names only (street addresses are too variable in format for edit distance to be meaningful).

### Soundex Match (`first_soundex_match`, `last_soundex_match`)

**What it measures**: Binary — do both strings produce the same Soundex code?

**How Soundex works**: Retains the first letter, then maps consonants to digits (B/F/P/V → 1, C/G/J/K/Q/S/X/Z → 2, etc.), drops vowels/H/W/Y, truncates to 4 characters. Example: `SMITH` → `S530`, `SMYTH` → `S530` → **match**.

**Why chosen**: Handles phonetic equivalents that character-level metrics miss entirely. `SMITH` and `SMYTH` score only 0.87 on JW but 1.0 on Soundex. Critical for names with historical spelling variants common in medical records.

**Range**: {0, 1} binary

**Limitation**: Very coarse — many non-matching names share Soundex codes (e.g., `LEE` and `LAW` both → `L000`). This is why it's used as a feature, not a standalone matcher.

### Metaphone Match (`first_metaphone_match`, `last_metaphone_match`)

**What it measures**: Binary — do both strings produce the same Metaphone code?

**How Metaphone works**: More sophisticated phonetic algorithm than Soundex. Handles silent letters, `PH` → `F`, `CK` → `K`, initial `KN`/`GN`/`PN` → `N`, etc. Produces variable-length codes.

**Why chosen**: Complements Soundex with better handling of complex phonetic patterns. Where Soundex groups `CATHERINE` and `CATHRYN` identically (both `C365`), Metaphone also correctly groups `KATHY` with them. Having both phonetic systems as separate features gives the classifier two independent phonetic signals.

**Range**: {0, 1} binary

### Exact Match Features (`city_match`, `state_match`, `zip5_match`)

**What they measure**: Binary — are the values identical after uppercasing and stripping?

**Why chosen**: Geographic features serve as **confirmation signals** rather than primary matchers. Two providers named `MICHAEL SMITH` in different states are almost certainly different people; in the same ZIP code, they're likely the same person. These features resolve ambiguity that name similarity alone cannot.

**Key statistics**:
- `state_match` = 1.000 (always 1 due to state-based blocking — excluded from ML as zero-variance)
- `city_match` = 0.051 (5.1% of pairs share a city — rare but discriminative)
- `zip5_match` = 0.011 (1.1% of pairs share a ZIP — very rare but extremely discriminative)

### Street Jaro-Winkler (`street_jw`)

**What it measures**: JW similarity on street address line 1.

**Why used with JW (not Levenshtein)**: Street addresses have highly variable formatting (`123 Main St Ste 200` vs `123 MAIN STREET SUITE 200`). JW's prefix bonus handles the common pattern where the street number and name match but suite/floor notation varies. Levenshtein would over-penalize these formatting differences.

**Mean**: 0.545 — moderate baseline similarity because many addresses share common prefixes (`123`, `100`, street names like `MAIN`, `OAK`).

---

## Composite Features: Why and How

### `name_avg` = (`first_jw` + `last_jw`) / 2

**Rationale**: Collapses two name dimensions into a single score for the classifier. A pair with `first_jw = 0.95` and `last_jw = 0.70` (name_avg = 0.825) is qualitatively different from `first_jw = 0.70` and `last_jw = 0.95` (same name_avg = 0.825) — but both represent "one strong name match + one moderate match." The composite captures this aggregate signal while the individual features preserve the asymmetry.

**XGBoost finding**: `name_avg` alone carries **96.0%** of XGBoost's feature importance — the boosted model discovered that this single composite is nearly sufficient to replicate the rule-based classifier.

### `addr_avg` = (`street_jw` + `city_match` + `zip5_match`) / 3

**Rationale**: Combines three geographic signals of varying granularity. ZIP match (most specific) + city match (moderate) + street similarity (format-sensitive) into one address confidence score. Pairs with `addr_avg` > 0.67 have at least two geographic confirmations.

### `raw_score` = (`name_avg` + `addr_avg`) / 2

**Rationale**: A single "overall linkage confidence" score. Useful for quick ranking of all 492K pairs and for setting coarse thresholds during exploratory analysis. Not used as a primary classification feature — the five-path rules operate on individual features, not composites.

### `name_len_ratio` = min(len(first_op), len(first_med)) / max(len(first_op), len(first_med))

**Rationale**: Cheap signal for detecting abbreviation mismatches. `ROB` vs `ROBERT` has JW = 0.94 (high) but name_len_ratio = 0.50 (low) — the length ratio flags that the "match" may be an abbreviation rather than a true identical name. This prevents false positives from short nicknames matching long formal names.

### `full_name_jw` = JW(first + " " + last, first + " " + last)

**Rationale**: Handles swapped names and hyphenated patterns. If OP has `JEAN MARIE SMITH` and Medicare has `MARIE SMITH-JEAN`, computing JW on individual first/last names fails, but the concatenated full name JW captures the overall character overlap.

---

## The Five-Path Rule Classifier

### Why Rules Instead of (Only) ML?

1. **Interpretability**: Each match can be traced to a specific path with explicit thresholds — critical for audit trails in healthcare data linkage
2. **No training data**: There is no external ground truth for tier-2 fuzzy matches. Rules encode domain knowledge about what constitutes a valid physician match
3. **Precision-first design**: Each path requires multiple simultaneous conditions to fire — false positives require multiple coincidental similarities, which is extremely unlikely

### Path Definitions

| Path | Logic | Intuition | Matches |
|------|-------|-----------|---------|
| **A** | `first_jw >= 0.85` AND `last_jw >= 0.85` AND (`zip5_match` OR `street_jw >= 0.80`) | Both names fuzzy-strong + address anchor | 103 |
| **B** | `last_lev = 1.0` AND `first_lev >= 0.60` AND `zip5_match` AND `city_match` | Exact surname + full geographic lock | 3 |
| **C2** | `first_jw >= 0.92` AND `first_lev >= 0.75` AND `last_jw = 1.0` AND (`zip5_match` OR `city_match`) | Very high first-name confidence + exact surname | 43 |
| **D** | `first_lev = 1.0` AND `last_lev = 1.0` AND `state_match` AND (if `fls_count >= 3`: also `city_match` OR `zip5_match`) | Exact full name, rarity-gated | 333 |
| **E** | `last_lev = 1.0` AND `first_jw >= 0.90` AND `first_lev >= 0.80` AND `city_match` AND `state_match` | Strong first name + city/state lock | 0 (subsumed by D) |

### Why Five Paths Instead of One?

Different match scenarios require different evidence combinations:

- **Path D** (333 matches, 69% of total): Handles the common case — exact name match. The **name rarity gate** (`fls_count`) is critical: `JOHN SMITH` in TX could match many Medicare providers, so Path D requires city or ZIP confirmation for common names. `XIOMARA BRECHBUHLER` in VT is almost certainly unique, so state match alone suffices.

- **Path A** (103 matches, 21%): Catches fuzzy name matches where both names are close but not exact. Requires geographic backup (ZIP or strong street match) to compensate for the name uncertainty.

- **Path C2** (43 matches, 9%): Targets a specific pattern — exact last name with a very strong (but not exact) first name match. Covers cases like `CHRISTOPHER` → `CHRIS` where `first_lev` is low but `first_jw` is very high.

- **Path B** (3 matches): Rare — exact last name + moderate first name, fully locked by geography. Fires only when ZIP AND city both match.

### The `possible` Tier

76 pairs miss all match paths but show reasonable similarity:
- `first_jw >= 0.65` AND `first_lev >= 0.60` AND `last_jw >= 0.90`
- Plus at least one geographic anchor (ZIP, city, or strong street match)

These are candidates for ML probability scoring or human review.

### Path Priority (np.select)

When multiple paths fire for the same pair, `np.select` assigns the **first qualifying path** (A → B → C2 → D → E). This prevents double-counting and ensures deterministic assignment.

---

## Ground Truth Construction

### The Challenge
There is **no external ground truth** for tier-2 fuzzy matches. We construct a proxy:

**Proxy ground truth**: Tier-2 OP records whose exact (first name, last name, state) triple exists in Medicare.

| Metric | Value |
|--------|-------|
| Tier-2 OP records with exact FLS in Medicare | **381** |
| Ground truth pairs (OP x all matching Medicare records) | **526** |

### Why This Works (and Its Limitations)

**Strengths**:
- Exact FLS match is a strong signal — if `SARAH JOHNSON` in `CA` exists in both datasets, they're very likely the same provider
- Provides a concrete, reproducible baseline

**Limitations**:
- **Conservative ceiling**: Only 381 of 4,683 tier-2 records (8.1%) have exact FLS matches in Medicare. The remaining 91.9% are genuinely ambiguous or have name variations
- **Not exhaustive**: True matches with name variations (`SARAH` / `SARA`) are excluded from ground truth
- **Assumes no homonyms**: Two different `SARAH JOHNSON` physicians in `CA` would be incorrectly treated as interchangeable

### Recall Ceiling Analysis

| Lookup Level | OP Coverage | Interpretation |
|-------------|------------|----------------|
| Last name in Medicare | 4,082 (87.2%) | Most OP providers share a surname with at least one Medicare provider |
| First + Last in Medicare | 1,300 (27.8%) | Roughly 1 in 4 have a full name match somewhere in Medicare |
| First + Last + State | **381 (8.1%)** | Only 8.1% are findable by exact FLS — the theoretical ceiling |
| **Current link rate** | **393 (8.4%)** | Exceeds ceiling because fuzzy paths find matches exact lookup misses |
| **Gap to close** | **0.1 pp** | The pipeline is already at the ceiling |

---

## Evaluation Metrics

### Record-Level Recall
**Question**: Of the 381 ground-truth OP records, how many did our classifier find?

| Tier | Found | Rate |
|------|-------|------|
| Match tier | 374 | **98.2%** |
| Possible tier | 17 | — |
| Match OR Possible | 374 | **98.2%** |
| Missed (false negatives) | 7 | 1.8% |

The 7 false negatives are all common names in large states: `JOHN LEE` (PA), `LINH NGUYEN` (CA), `ANDREW JOHNSON` (MI), `RYAN JOHNSON` (FL), `RICHARD KIM` (CA), `DAVID LEE` (CA), `DAVID SMITH` (CA). These fail Path D's rarity gate and lack the geographic specificity needed for Paths A/B/C2.

### Pair-Level Precision
**Question**: Of the 482 match pairs our classifier produced, how many point to the *correct* Medicare record?

| Metric | Value |
|--------|-------|
| Our match pairs | 482 |
| Ground truth pairs | 526 |
| Correct pairs (TP) | 456 |
| **Pair-level precision** | **94.6%** |

### Record-Level Precision (Lower Bound)

| Metric | Value |
|--------|-------|
| Linked OP records | 438 |
| In ground truth | 374 |
| Not in ground truth | 64 |
| **Precision lower bound** | **85.4%** |

The 64 "not in ground truth" records are not necessarily wrong — they may be correct fuzzy matches for OP records whose exact FLS doesn't appear in Medicare (the ground truth is incomplete, not the classifier).

---

## Statistical Properties of the Feature Space

### Feature Correlations

The 15 ML features exhibit expected correlation structure:
- **High correlation clusters**: (`first_jw`, `first_lev`, `first_soundex_match`, `first_metaphone_match`) — all measure first name similarity through different lenses
- **Cross-domain independence**: Name features are largely uncorrelated with address features
- **Composite correlation**: `name_avg`, `raw_score`, and `full_name_jw` are mechanically correlated with their component features — expected and acceptable for tree-based models

### Class Separation

The feature distributions show near-perfect class separation for the match tier:
- **Matches** (n=482): Concentrated at 1.0 for name JW/Lev features, with high city/ZIP match rates
- **Non-matches** (n=491,869): Broad distributions centered at 0.4-0.6 for name features, near 0 for geographic features
- **Possible** (n=76): Intermediate — moderate name scores (0.65-0.90) with some geographic confirmation

![Similarity Score Distributions by Match Tier](similarity_distributions.jpg)

### Score Distribution Thresholds

| Threshold | Pairs Above | % of Total | Interpretation |
|-----------|-------------|-----------|----------------|
| `raw_score >= 0.80` | 1,529 | 0.31% | Very high confidence candidates |
| `raw_score >= 0.70` | 4,035 | 0.82% | High confidence |
| `raw_score >= 0.60` | 12,163 | 2.47% | Moderate confidence |
| `name_avg >= 0.90` | 2,598 | 0.53% | Strong name match |

---

## Why This Statistical Approach Works

### Design Principles

1. **Multi-metric redundancy**: No single metric is trusted alone. Every match path requires >= 2 independent similarity signals, reducing false positive risk multiplicatively.

2. **Precision-first with recall monitoring**: The pipeline accepts missing some matches (1.8% false negative rate) to avoid linking wrong providers (5.4% pair error rate). In healthcare data, a wrong link is far more harmful than a missing link.

3. **Rarity-aware thresholds**: Path D's `fls_count` gate adjusts evidence requirements based on name frequency in the population. Common names need more geographic evidence; rare names need less. This is a form of **adaptive thresholding** driven by the data distribution.

4. **Feature engineering over model complexity**: The 15-feature vector captures name, phonetic, geographic, and composite signals. This rich representation lets even a simple rule-based classifier achieve 98.2% recall / 94.6% precision — proving that feature engineering matters more than model sophistication for this problem.

5. **Honest evaluation despite no ground truth**: The proxy ground truth (exact FLS) is explicitly acknowledged as conservative. The recall ceiling analysis (8.1% vs 8.4% link rate) proves the pipeline isn't just working — it's exceeding the theoretical ceiling of deterministic matching.
