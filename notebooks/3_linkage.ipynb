{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 — Blocking & Candidate Generation\n",
    "\n",
    "**Input:** Cleaned parquet files from Phase 2 (`../artifacts/phase2_preprocessing/`)  \n",
    "**Output:** Deduplicated candidate pairs + blocking summary (`../artifacts/phase3_blocking/`)\n",
    "\n",
    "### Approach\n",
    "1. **Information-theoretic analysis** — pick the best blocking keys using entropy & mutual information\n",
    "2. **Traditional blocking** — Strategies A, B, C (increasing specificity)\n",
    "3. **Advanced blocking** — Canopy clustering (TF-IDF + cosine) and LSH (MinHash)\n",
    "4. **Union + evaluation** — combine all strategies, measure quality, export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 — Load Cleaned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1 — LOAD CLEANED DATASETS\n",
      "------------------------------------------------------------\n",
      "OP tier2_fuzzy records:         4,683\n",
      "Medicare records:           1,175,281\n",
      "PECOS records:              2,936,748\n",
      "Full cross-product:      5,503,840,923 pairs\n",
      "\n",
      "✓ Datasets loaded.\n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('3.1 — LOAD CLEANED DATASETS')\n",
    "print('-' * 60)\n",
    "\n",
    "INPUT_DIR  = '../artifacts/phase2_preprocessing'\n",
    "OUTPUT_DIR = '../artifacts/phase3_blocking'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "op_clean    = pd.read_parquet(os.path.join(INPUT_DIR, 'open_payments_clean.parquet'))\n",
    "med_clean   = pd.read_parquet(os.path.join(INPUT_DIR, 'medicare_clean.parquet'))\n",
    "pecos_clean = pd.read_parquet(os.path.join(INPUT_DIR, 'pecos_clean.parquet'))\n",
    "\n",
    "op_tier2 = op_clean[op_clean['linkage_tier'] == 'tier2_fuzzy'].copy().reset_index(drop=True)\n",
    "med_clean = med_clean.reset_index(drop=True)\n",
    "\n",
    "full_cross = len(op_tier2) * len(med_clean)\n",
    "\n",
    "print(f'OP tier2_fuzzy records:  {len(op_tier2):>12,}')\n",
    "print(f'Medicare records:        {len(med_clean):>12,}')\n",
    "print(f'PECOS records:           {len(pecos_clean):>12,}')\n",
    "print(f'Full cross-product:      {full_cross:>12,} pairs')\n",
    "\n",
    "del op_clean; gc.collect()\n",
    "print('\\n✓ Datasets loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 — Blocking Key Selection via Information Theory\n",
    "\n",
    "Before picking blocking keys, we need a **data-driven way** to decide which\n",
    "attributes to combine. Two simple ideas from information theory help:\n",
    "\n",
    "- **Entropy** — How spread out are the values? An attribute with many distinct\n",
    "  values (high entropy) creates smaller, tighter blocks. State has ~53 values\n",
    "  (low entropy), while Last Name Soundex has thousands (high entropy).\n",
    "\n",
    "- **Mutual Information** — How much do two attributes overlap? If State and\n",
    "  Soundex tell you very *different* things about a record (low MI), combining\n",
    "  them gives you a much better blocking key. If they're redundant (high MI),\n",
    "  combining them doesn't help much.\n",
    "\n",
    "We compute these for every candidate attribute, then pick the combination\n",
    "with the **highest joint entropy** (most distinct blocks) and **lowest MI**\n",
    "(least redundancy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2 — BLOCKING KEY SELECTION VIA INFORMATION THEORY\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Single-Attribute Entropy (bits) ---\n",
      "Attribute        Entropy  Unique Vals\n",
      "-------------------------------------\n",
      "State              4.586           53\n",
      "LN Soundex         9.868        1,611\n",
      "FN Soundex         8.282          764\n",
      "Last Name         11.319        3,385\n",
      "First Name         9.746        1,757\n",
      "ZIP5              10.715        2,540\n",
      "\n",
      "--- Pairwise Mutual Information (bits) ---\n",
      "(Lower = more independent = better to combine)\n",
      "                    State LN Sounde FN Sounde Last Name First Nam      ZIP5\n",
      "State                   —     2.680     1.610     3.858     2.594     4.572\n",
      "LN Soundex          2.680         —     6.017     9.868     7.451     8.414\n",
      "FN Soundex          1.610     6.017         —     7.434     8.282     6.845\n",
      "Last Name           3.858     9.868     7.434         —     8.887     9.845\n",
      "First Name          2.594     7.451     8.282     8.887         —     8.289\n",
      "ZIP5                4.572     8.414     6.845     9.845     8.289         —\n",
      "\n",
      "--- Composite Key Joint Entropy ---\n",
      "(Higher = more distinct blocks = better reduction)\n",
      "Key Combination                                Entropy     Blocks\n",
      "-----------------------------------------------------------------\n",
      "State + LN Soundex                              11.774      3,895\n",
      "State + Last Name                               12.048      4,398\n",
      "State + FN Soundex + LN Soundex                 12.184      4,662\n",
      "State + Last Name + First Name                  12.189      4,672\n",
      "State + ZIP5                                    10.718      2,548\n",
      "\n",
      "--- Independence Check ---\n",
      "Ratio close to 1.0 = independent (good to combine)\n",
      "  State + LN Soundex: independence ratio = 0.8146\n",
      "  FN Soundex + LN Soundex: independence ratio = 0.6685\n",
      "  State + FN Soundex: independence ratio = 0.8749\n",
      "\n",
      "✓ Analysis complete.\n",
      "  → State + LN Soundex: high joint entropy, low MI → best 2-key combo (Strategy A)\n",
      "  → Adding FN Soundex: further tightens blocks with minimal redundancy (Strategy C)\n"
     ]
    }
   ],
   "source": [
    "print('3.2 — BLOCKING KEY SELECTION VIA INFORMATION THEORY')\n",
    "print('-' * 60)\n",
    "\n",
    "def shannon_entropy(series):\n",
    "    \"\"\"Entropy in bits — higher means more distinct values.\"\"\"\n",
    "    counts = series.dropna().value_counts()\n",
    "    probs = counts / counts.sum()\n",
    "    return -(probs * np.log2(probs)).sum()\n",
    "\n",
    "def joint_entropy(s1, s2):\n",
    "    \"\"\"Joint entropy of two attributes combined.\"\"\"\n",
    "    mask = s1.notna() & s2.notna()\n",
    "    combined = s1[mask].astype(str) + '||' + s2[mask].astype(str)\n",
    "    return shannon_entropy(combined)\n",
    "\n",
    "def mutual_information(s1, s2):\n",
    "    \"\"\"MI = how much knowing one tells you about the other.\"\"\"\n",
    "    mask = s1.notna() & s2.notna()\n",
    "    return shannon_entropy(s1[mask]) + shannon_entropy(s2[mask]) - joint_entropy(s1, s2)\n",
    "\n",
    "# Candidate blocking attributes (OP side)\n",
    "op_attrs = {\n",
    "    'State':      op_tier2['Recipient_State'],\n",
    "    'LN Soundex': op_tier2['LAST_NAME_SOUNDEX'],\n",
    "    'FN Soundex': op_tier2['FIRST_NAME_SOUNDEX'],\n",
    "    'Last Name':  op_tier2['Covered_Recipient_Last_Name'].str.upper(),\n",
    "    'First Name': op_tier2['Covered_Recipient_First_Name'].str.upper(),\n",
    "    'ZIP5':       op_tier2['Recipient_Zip5'],\n",
    "}\n",
    "\n",
    "# 1) Single-attribute entropy\n",
    "print('\\n--- Single-Attribute Entropy (bits) ---')\n",
    "print(f\"{'Attribute':<15} {'Entropy':>8} {'Unique Vals':>12}\")\n",
    "print('-' * 37)\n",
    "for name, series in op_attrs.items():\n",
    "    print(f'{name:<15} {shannon_entropy(series):>8.3f} {series.dropna().nunique():>12,}')\n",
    "\n",
    "# 2) Pairwise MI\n",
    "print('\\n--- Pairwise Mutual Information (bits) ---')\n",
    "print('(Lower = more independent = better to combine)')\n",
    "names = list(op_attrs.keys())\n",
    "header = f\"{'':>15}\"\n",
    "for a in names: header += f' {a[:9]:>9}'\n",
    "print(header)\n",
    "for a in names:\n",
    "    row = f'{a:<15}'\n",
    "    for b in names:\n",
    "        if a == b: row += f\" {'—':>9}\"\n",
    "        else: row += f' {mutual_information(op_attrs[a], op_attrs[b]):>9.3f}'\n",
    "    print(row)\n",
    "\n",
    "# 3) Composite key joint entropy\n",
    "print('\\n--- Composite Key Joint Entropy ---')\n",
    "print('(Higher = more distinct blocks = better reduction)')\n",
    "combos = [\n",
    "    ('State', 'LN Soundex'),\n",
    "    ('State', 'Last Name'),\n",
    "    ('State', 'FN Soundex', 'LN Soundex'),\n",
    "    ('State', 'Last Name', 'First Name'),\n",
    "    ('State', 'ZIP5'),\n",
    "]\n",
    "print(f\"{'Key Combination':<45} {'Entropy':>8} {'Blocks':>10}\")\n",
    "print('-' * 65)\n",
    "for combo in combos:\n",
    "    parts = [op_attrs[c].fillna('').astype(str) for c in combo]\n",
    "    combined = parts[0]\n",
    "    for p in parts[1:]: combined = combined + '||' + p\n",
    "    print(f\"{' + '.join(combo):<45} {shannon_entropy(combined):>8.3f} {combined.nunique():>10,}\")\n",
    "\n",
    "# 4) Independence check\n",
    "print('\\n--- Independence Check ---')\n",
    "print('Ratio close to 1.0 = independent (good to combine)')\n",
    "for a, b in [('State','LN Soundex'),('FN Soundex','LN Soundex'),('State','FN Soundex')]:\n",
    "    h_a, h_b = shannon_entropy(op_attrs[a]), shannon_entropy(op_attrs[b])\n",
    "    h_ab = joint_entropy(op_attrs[a], op_attrs[b])\n",
    "    ratio = h_ab / (h_a + h_b) if (h_a + h_b) > 0 else 0\n",
    "    print(f'  {a} + {b}: independence ratio = {ratio:.4f}')\n",
    "\n",
    "print('\\n✓ Analysis complete.')\n",
    "print('  → State + LN Soundex: high joint entropy, low MI → best 2-key combo (Strategy A)')\n",
    "print('  → Adding FN Soundex: further tightens blocks with minimal redundancy (Strategy C)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 — Strategy A: State + Last Name Soundex\n",
    "\n",
    "Justified by 3.2: State and LN Soundex are near-independent with high joint\n",
    "entropy — the best 2-attribute blocking key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3 — STRATEGY A: STATE + LAST NAME SOUNDEX\n",
      "------------------------------------------------------------\n",
      "Candidate pairs (A):       427,752\n",
      "Reduction ratio:      99.992228%\n",
      "✓ Strategy A complete.\n"
     ]
    }
   ],
   "source": [
    "print('3.3 — STRATEGY A: STATE + LAST NAME SOUNDEX')\n",
    "print('-' * 60)\n",
    "\n",
    "op_tier2['_block_A'] = op_tier2['LAST_NAME_SOUNDEX'].fillna('') + '_' + op_tier2['Recipient_State'].fillna('')\n",
    "med_clean['_block_A'] = med_clean['LAST_NAME_SOUNDEX'].fillna('') + '_' + med_clean['Rndrng_Prvdr_State_Abrvtn'].fillna('')\n",
    "\n",
    "pairs_A = (\n",
    "    op_tier2[['_block_A']].reset_index().rename(columns={'index':'index_op'})\n",
    "    .merge(med_clean[['_block_A']].reset_index().rename(columns={'index':'index_med'}), on='_block_A')\n",
    "    [['index_op','index_med']]\n",
    ")\n",
    "\n",
    "rr_A = 1 - len(pairs_A) / full_cross\n",
    "print(f'Candidate pairs (A):  {len(pairs_A):>12,}')\n",
    "print(f'Reduction ratio:      {rr_A*100:.6f}%')\n",
    "op_tier2.drop(columns='_block_A', inplace=True)\n",
    "med_clean.drop(columns='_block_A', inplace=True)\n",
    "gc.collect()\n",
    "print('✓ Strategy A complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 — Strategy B: Exact Last Name (upper) + State\n",
    "\n",
    "Stricter than A — uses exact uppercased last name instead of Soundex.\n",
    "More unique values (higher entropy), avoids phonetic collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4 — STRATEGY B: EXACT LAST NAME + STATE\n",
      "------------------------------------------------------------\n",
      "Candidate pairs (B):       118,742\n",
      "Reduction ratio:      99.997843%\n",
      "✓ Strategy B complete.\n"
     ]
    }
   ],
   "source": [
    "print('3.4 — STRATEGY B: EXACT LAST NAME + STATE')\n",
    "print('-' * 60)\n",
    "\n",
    "op_tier2['_block_B'] = op_tier2['Covered_Recipient_Last_Name'].fillna('').str.upper() + '_' + op_tier2['Recipient_State'].fillna('')\n",
    "med_clean['_block_B'] = med_clean['Rndrng_Prvdr_Last_Org_Name'].fillna('').str.upper() + '_' + med_clean['Rndrng_Prvdr_State_Abrvtn'].fillna('')\n",
    "\n",
    "pairs_B = (\n",
    "    op_tier2[['_block_B']].reset_index().rename(columns={'index':'index_op'})\n",
    "    .merge(med_clean[['_block_B']].reset_index().rename(columns={'index':'index_med'}), on='_block_B')\n",
    "    [['index_op','index_med']]\n",
    ")\n",
    "\n",
    "rr_B = 1 - len(pairs_B) / full_cross\n",
    "print(f'Candidate pairs (B):  {len(pairs_B):>12,}')\n",
    "print(f'Reduction ratio:      {rr_B*100:.6f}%')\n",
    "op_tier2.drop(columns='_block_B', inplace=True)\n",
    "med_clean.drop(columns='_block_B', inplace=True)\n",
    "gc.collect()\n",
    "print('✓ Strategy B complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 — Strategy C: First Name Soundex + Last Name Soundex + State\n",
    "\n",
    "Tightest 3-part key. The independence check in 3.2 confirms FN Soundex\n",
    "and LN Soundex carry different information, so combining them adds\n",
    "real discriminative power without redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 — STRATEGY C: FN SOUNDEX + LN SOUNDEX + STATE\n",
      "------------------------------------------------------------\n",
      "Candidate pairs (C):         2,795\n",
      "Reduction ratio:      99.999949%\n",
      "✓ Strategy C complete.\n"
     ]
    }
   ],
   "source": [
    "print('3.5 — STRATEGY C: FN SOUNDEX + LN SOUNDEX + STATE')\n",
    "print('-' * 60)\n",
    "\n",
    "op_tier2['_block_C'] = (\n",
    "    op_tier2['FIRST_NAME_SOUNDEX'].fillna('') + '_' +\n",
    "    op_tier2['LAST_NAME_SOUNDEX'].fillna('') + '_' +\n",
    "    op_tier2['Recipient_State'].fillna('')\n",
    ")\n",
    "med_clean['_block_C'] = (\n",
    "    med_clean['FIRST_NAME_SOUNDEX'].fillna('') + '_' +\n",
    "    med_clean['LAST_NAME_SOUNDEX'].fillna('') + '_' +\n",
    "    med_clean['Rndrng_Prvdr_State_Abrvtn'].fillna('')\n",
    ")\n",
    "\n",
    "pairs_C = (\n",
    "    op_tier2[['_block_C']].reset_index().rename(columns={'index':'index_op'})\n",
    "    .merge(med_clean[['_block_C']].reset_index().rename(columns={'index':'index_med'}), on='_block_C')\n",
    "    [['index_op','index_med']]\n",
    ")\n",
    "\n",
    "rr_C = 1 - len(pairs_C) / full_cross\n",
    "print(f'Candidate pairs (C):  {len(pairs_C):>12,}')\n",
    "print(f'Reduction ratio:      {rr_C*100:.6f}%')\n",
    "op_tier2.drop(columns='_block_C', inplace=True)\n",
    "med_clean.drop(columns='_block_C', inplace=True)\n",
    "gc.collect()\n",
    "print('✓ Strategy C complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 — Canopy Clustering Blocking\n",
    "\n",
    "Canopy clustering is a **two-threshold** blocking method:\n",
    "- Convert each record's `name + state` into a TF-IDF vector (character 3-grams)\n",
    "- For each OP record, find all Medicare records within cosine distance **T2** (tight = 0.4)\n",
    "- Records within **T1** (loose = 0.6) get removed from the candidate pool to avoid redundant centers\n",
    "\n",
    "This catches fuzzy name matches that exact blocking misses, without comparing\n",
    "every single pair. Processed per-state to keep memory manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6 — CANOPY CLUSTERING BLOCKING\n",
      "------------------------------------------------------------\n",
      "  10/53 states, pairs so far: 8,321\n",
      "  20/53 states, pairs so far: 9,505\n",
      "  30/53 states, pairs so far: 11,174\n",
      "  40/53 states, pairs so far: 14,238\n",
      "  50/53 states, pairs so far: 18,738\n",
      "  53/53 states, pairs so far: 18,815\n",
      "\n",
      "Canopy pairs:        18,815\n",
      "Reduction:     99.999658%\n",
      "Thresholds:    T1=0.6, T2=0.4\n",
      "✓ Canopy complete.\n"
     ]
    }
   ],
   "source": [
    "print('3.6 — CANOPY CLUSTERING BLOCKING')\n",
    "print('-' * 60)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "T1 = 0.6\n",
    "T2 = 0.4\n",
    "\n",
    "def name_str(first, last, state):\n",
    "    parts = [str(x).strip().upper() for x in [first, last, state] if pd.notna(x)]\n",
    "    return ' '.join(p for p in parts if p and p != 'NAN')\n",
    "\n",
    "op_tier2['_can'] = op_tier2.apply(lambda r: name_str(r['Covered_Recipient_First_Name'], r['Covered_Recipient_Last_Name'], r['Recipient_State']), axis=1)\n",
    "med_clean['_can'] = med_clean.apply(lambda r: name_str(r['Rndrng_Prvdr_First_Name'], r['Rndrng_Prvdr_Last_Org_Name'], r['Rndrng_Prvdr_State_Abrvtn']), axis=1)\n",
    "\n",
    "states_common = sorted(set(op_tier2['Recipient_State'].dropna().unique()) & set(med_clean['Rndrng_Prvdr_State_Abrvtn'].dropna().unique()))\n",
    "canopy_list = []\n",
    "\n",
    "for si, state in enumerate(states_common):\n",
    "    op_st  = op_tier2[op_tier2['Recipient_State'] == state]\n",
    "    med_st = med_clean[med_clean['Rndrng_Prvdr_State_Abrvtn'] == state]\n",
    "    if len(op_st) == 0 or len(med_st) == 0: continue\n",
    "\n",
    "    all_str = pd.concat([op_st['_can'].reset_index(drop=True), med_st['_can'].reset_index(drop=True)], ignore_index=True)\n",
    "    n_op = len(op_st)\n",
    "\n",
    "    tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3,3))\n",
    "    try: mat = tfidf.fit_transform(all_str)\n",
    "    except ValueError: continue\n",
    "\n",
    "    dist = cosine_distances(mat[:n_op], mat[n_op:])\n",
    "    op_idx, med_idx = op_st.index.tolist(), med_st.index.tolist()\n",
    "\n",
    "    for i in range(n_op):\n",
    "        for j in np.where(dist[i] <= T2)[0]:\n",
    "            canopy_list.append((op_idx[i], med_idx[j]))\n",
    "\n",
    "    if (si+1) % 10 == 0 or (si+1) == len(states_common):\n",
    "        print(f'  {si+1}/{len(states_common)} states, pairs so far: {len(canopy_list):,}')\n",
    "\n",
    "pairs_canopy = pd.DataFrame(canopy_list, columns=['index_op','index_med']).drop_duplicates()\n",
    "rr_can = 1 - len(pairs_canopy) / full_cross\n",
    "print(f'\\nCanopy pairs:  {len(pairs_canopy):>12,}')\n",
    "print(f'Reduction:     {rr_can*100:.6f}%')\n",
    "print(f'Thresholds:    T1={T1}, T2={T2}')\n",
    "op_tier2.drop(columns='_can', inplace=True)\n",
    "med_clean.drop(columns='_can', inplace=True)\n",
    "del canopy_list; gc.collect()\n",
    "print('✓ Canopy complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 — LSH (Locality-Sensitive Hashing) Blocking\n",
    "\n",
    "LSH is a probabilistic method that finds **approximately similar** records\n",
    "without comparing every pair:\n",
    "\n",
    "1. Break each `name + state` string into small character chunks (3-grams)\n",
    "2. Create a compact fingerprint (MinHash signature) for each record\n",
    "3. Hash fingerprints into buckets — similar records land in the same bucket\n",
    "\n",
    "The similarity threshold is ~0.5 Jaccard. Records sharing roughly half\n",
    "their character 3-grams become candidates. This catches spelling variations\n",
    "that exact blocking misses entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasketch ready\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import datasketch; print('datasketch ready')\n",
    "except ImportError:\n",
    "    !pip install datasketch -q\n",
    "    import datasketch; print('datasketch installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7 — LSH BLOCKING\n",
      "------------------------------------------------------------\n",
      "States in common: 53\n",
      "  10/53 states, pairs so far: 45,651\n",
      "  20/53 states, pairs so far: 52,368\n",
      "  30/53 states, pairs so far: 61,172\n",
      "  40/53 states, pairs so far: 82,225\n",
      "  50/53 states, pairs so far: 99,900\n",
      "  53/53 states, pairs so far: 100,196\n",
      "\n",
      "LSH pairs:          100,196\n",
      "Reduction:     99.998180%\n",
      "✓ LSH complete.\n"
     ]
    }
   ],
   "source": [
    "print('3.7 — LSH BLOCKING')\n",
    "print('-' * 60)\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "NUM_PERM, LSH_THRESH, Q = 128, 0.5, 3\n",
    "\n",
    "def mk_minhash(text):\n",
    "    m = MinHash(num_perm=NUM_PERM)\n",
    "    if not isinstance(text, str) or len(text) < Q: return None\n",
    "    for i in range(len(text)-Q+1): m.update(text[i:i+Q].encode('utf-8'))\n",
    "    return m\n",
    "\n",
    "def ns(first, last, state):\n",
    "    parts = [str(x).strip().upper() for x in [first,last,state] if pd.notna(x)]\n",
    "    return ' '.join(p for p in parts if p and p != 'NAN')\n",
    "\n",
    "op_tier2['_lsh'] = op_tier2.apply(lambda r: ns(r['Covered_Recipient_First_Name'], r['Covered_Recipient_Last_Name'], r['Recipient_State']), axis=1)\n",
    "med_clean['_lsh'] = med_clean.apply(lambda r: ns(r['Rndrng_Prvdr_First_Name'], r['Rndrng_Prvdr_Last_Org_Name'], r['Rndrng_Prvdr_State_Abrvtn']), axis=1)\n",
    "\n",
    "states_common = sorted(set(op_tier2['Recipient_State'].dropna().unique()) & set(med_clean['Rndrng_Prvdr_State_Abrvtn'].dropna().unique()))\n",
    "print(f'States in common: {len(states_common)}')\n",
    "lsh_list = []\n",
    "\n",
    "for si, state in enumerate(states_common):\n",
    "    op_st  = op_tier2[op_tier2['Recipient_State'] == state]\n",
    "    med_st = med_clean[med_clean['Rndrng_Prvdr_State_Abrvtn'] == state]\n",
    "    if len(op_st)==0 or len(med_st)==0: continue\n",
    "\n",
    "    lsh = MinHashLSH(threshold=LSH_THRESH, num_perm=NUM_PERM)\n",
    "    for idx, row in med_st.iterrows():\n",
    "        mh = mk_minhash(row['_lsh'])\n",
    "        if mh:\n",
    "            try: lsh.insert(f'm_{idx}', mh)\n",
    "            except ValueError: pass\n",
    "\n",
    "    for idx_op, row_op in op_st.iterrows():\n",
    "        mh_op = mk_minhash(row_op['_lsh'])\n",
    "        if mh_op is None: continue\n",
    "        for key in lsh.query(mh_op):\n",
    "            lsh_list.append((idx_op, int(key.split('_')[1])))\n",
    "\n",
    "    if (si+1) % 10 == 0 or (si+1) == len(states_common):\n",
    "        print(f'  {si+1}/{len(states_common)} states, pairs so far: {len(lsh_list):,}')\n",
    "    del lsh; gc.collect()\n",
    "\n",
    "pairs_LSH = pd.DataFrame(lsh_list, columns=['index_op','index_med']).drop_duplicates()\n",
    "rr_LSH = 1 - len(pairs_LSH) / full_cross\n",
    "print(f'\\nLSH pairs:     {len(pairs_LSH):>12,}')\n",
    "print(f'Reduction:     {rr_LSH*100:.6f}%')\n",
    "op_tier2.drop(columns='_lsh', inplace=True)\n",
    "med_clean.drop(columns='_lsh', inplace=True)\n",
    "del lsh_list; gc.collect()\n",
    "print('✓ LSH complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 — Union All Strategies & Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8 — UNION & DEDUPLICATE\n",
      "------------------------------------------------------------\n",
      "Union total:       492,427 unique pairs\n",
      "  A             427,752 total |    427,752 unique to A\n",
      "  B             118,742 total |    118,742 unique to B\n",
      "  C               2,795 total |      2,795 unique to C\n",
      "  Canopy         18,815 total |     18,815 unique to Canopy\n",
      "  LSH           100,196 total |    100,196 unique to LSH\n",
      "✓ Union complete.\n"
     ]
    }
   ],
   "source": [
    "print('3.8 — UNION & DEDUPLICATE')\n",
    "print('-' * 60)\n",
    "\n",
    "set_A      = set(map(tuple, pairs_A[['index_op','index_med']].values))\n",
    "set_B      = set(map(tuple, pairs_B[['index_op','index_med']].values))\n",
    "set_C      = set(map(tuple, pairs_C[['index_op','index_med']].values))\n",
    "set_canopy = set(map(tuple, pairs_canopy[['index_op','index_med']].values))\n",
    "set_LSH    = set(map(tuple, pairs_LSH[['index_op','index_med']].values))\n",
    "set_all    = set_A | set_B | set_C | set_canopy | set_LSH\n",
    "\n",
    "all_pairs = pd.DataFrame(list(set_all), columns=['index_op','index_med'])\n",
    "print(f'Union total:  {len(all_pairs):>12,} unique pairs')\n",
    "\n",
    "for name, s in [('A',set_A),('B',set_B),('C',set_C),('Canopy',set_canopy),('LSH',set_LSH)]:\n",
    "    u = s - (set_all - s)\n",
    "    print(f'  {name:<10} {len(s):>10,} total | {len(u):>10,} unique to {name}')\n",
    "print('✓ Union complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 — Blocking Quality Metrics\n",
    "\n",
    "For each strategy: pair count, reduction ratio, OP coverage (how many\n",
    "OP records appear in at least one pair), and recall ceiling (% of\n",
    "\"findable\" OP records — those with an exact name+state match in Medicare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9 — QUALITY METRICS\n",
      "------------------------------------------------------------\n",
      "Findable OP records: 381 / 4,683\n",
      "\n",
      "Strategy          Pairs           RR %   OP Cov   Recall %\n",
      "========================================================\n",
      "A               427,752     99.992228%    4,574    100.00%\n",
      "B               118,742     99.997843%    3,033    100.00%\n",
      "C                 2,795     99.999949%    1,206    100.00%\n",
      "Canopy           18,815     99.999658%    2,562    100.00%\n",
      "LSH             100,196     99.998180%    3,720    100.00%\n",
      "Union           492,427     99.991053%    4,622    100.00%\n",
      "\n",
      "✓ Metrics complete.\n"
     ]
    }
   ],
   "source": [
    "print('3.9 — QUALITY METRICS')\n",
    "print('-' * 60)\n",
    "\n",
    "med_keys = set(zip(\n",
    "    med_clean['Rndrng_Prvdr_First_Name'].str.upper(),\n",
    "    med_clean['Rndrng_Prvdr_Last_Org_Name'].str.upper(),\n",
    "    med_clean['Rndrng_Prvdr_State_Abrvtn']\n",
    "))\n",
    "findable = {idx for idx, r in op_tier2.iterrows()\n",
    "    if (str(r['Covered_Recipient_First_Name']).upper(),\n",
    "        str(r['Covered_Recipient_Last_Name']).upper(),\n",
    "        str(r['Recipient_State'])) in med_keys}\n",
    "\n",
    "print(f'Findable OP records: {len(findable):,} / {len(op_tier2):,}\\n')\n",
    "\n",
    "strats = {'A':set_A, 'B':set_B, 'C':set_C, 'Canopy':set_canopy, 'LSH':set_LSH, 'Union':set_all}\n",
    "rows = []\n",
    "print(f\"{'Strategy':<10} {'Pairs':>12} {'RR %':>14} {'OP Cov':>8} {'Recall %':>10}\")\n",
    "print('=' * 56)\n",
    "for name, ps in strats.items():\n",
    "    n = len(ps)\n",
    "    rr = (1 - n/full_cross)*100\n",
    "    op_in = {p[0] for p in ps}\n",
    "    recall = len(findable & op_in)/len(findable)*100 if findable else 0\n",
    "    print(f'{name:<10} {n:>12,} {rr:>13.6f}% {len(op_in):>8,} {recall:>9.2f}%')\n",
    "    rows.append({'strategy':name,'pairs':n,'reduction_ratio':1-n/full_cross,'op_coverage':len(op_in),'recall_ceiling_pct':round(recall,2)})\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "print('\\n✓ Metrics complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 — Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10 — EXPORT\n",
      "------------------------------------------------------------\n",
      "✓ candidate_pairs.parquet (492,427 pairs)\n",
      "✓ blocking_summary.csv (6 rows)\n",
      "\n",
      "All artifacts → ../artifacts/phase3_blocking/\n",
      "  blocking_summary.csv                    0.00 MB\n",
      "  candidate_pairs.parquet                 3.16 MB\n",
      "\n",
      "============================================================\n",
      "PHASE 3 COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print('3.10 — EXPORT')\n",
    "print('-' * 60)\n",
    "\n",
    "all_pairs.to_parquet(os.path.join(OUTPUT_DIR, 'candidate_pairs.parquet'), index=False)\n",
    "print(f'✓ candidate_pairs.parquet ({len(all_pairs):,} pairs)')\n",
    "\n",
    "summary_df.to_csv(os.path.join(OUTPUT_DIR, 'blocking_summary.csv'), index=False)\n",
    "print(f'✓ blocking_summary.csv ({len(summary_df)} rows)')\n",
    "\n",
    "print(f'\\nAll artifacts → {OUTPUT_DIR}/')\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    sz = os.path.getsize(os.path.join(OUTPUT_DIR, f)) / 1024 / 1024\n",
    "    print(f'  {f:<35s} {sz:>8.2f} MB')\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('PHASE 3 COMPLETE')\n",
    "print('=' * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
