{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76badc89",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing & Standardization\n",
    "\n",
    "Goal: Build reusable cleaning functions for names, addresses, and identifiers\n",
    "that will feed into blocking and matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a7dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530ed2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "medicare_path = \"../data/MUP_PHY_R25_P05_V20_D23_Prov_Svc.csv\"\n",
    "open_payments_path = \"../data/OP_DTL_GNRL_PGYR2023_P01232026_01102026.csv\"\n",
    "pecos_path = \"../data/Medicare_FFS_Public_Provider_Enrollment_Q3_2025.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5e3493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PECOS: 2,936,748 rows, 11 columns\n",
      "Index(['NPI', 'MULTIPLE_NPI_FLAG', 'PECOS_ASCT_CNTL_ID', 'ENRLMT_ID',\n",
      "       'PROVIDER_TYPE_CD', 'PROVIDER_TYPE_DESC', 'STATE_CD', 'FIRST_NAME',\n",
      "       'MDL_NAME', 'LAST_NAME', 'ORG_NAME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "pecos_df = pd.read_csv(pecos_path, low_memory=False, encoding='latin')\n",
    "print(f\"  PECOS: {len(pecos_df):,} rows, {len(pecos_df.columns)} columns\")\n",
    "print(pecos_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eed1575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Medicare: 9,660,647 rows, 28 columns\n",
      "Index(['Rndrng_NPI', 'Rndrng_Prvdr_Last_Org_Name', 'Rndrng_Prvdr_First_Name',\n",
      "       'Rndrng_Prvdr_MI', 'Rndrng_Prvdr_Crdntls', 'Rndrng_Prvdr_Ent_Cd',\n",
      "       'Rndrng_Prvdr_St1', 'Rndrng_Prvdr_St2', 'Rndrng_Prvdr_City',\n",
      "       'Rndrng_Prvdr_State_Abrvtn', 'Rndrng_Prvdr_State_FIPS',\n",
      "       'Rndrng_Prvdr_Zip5', 'Rndrng_Prvdr_RUCA', 'Rndrng_Prvdr_RUCA_Desc',\n",
      "       'Rndrng_Prvdr_Cntry', 'Rndrng_Prvdr_Type',\n",
      "       'Rndrng_Prvdr_Mdcr_Prtcptg_Ind', 'HCPCS_Cd', 'HCPCS_Desc',\n",
      "       'HCPCS_Drug_Ind', 'Place_Of_Srvc', 'Tot_Benes', 'Tot_Srvcs',\n",
      "       'Tot_Bene_Day_Srvcs', 'Avg_Sbmtd_Chrg', 'Avg_Mdcr_Alowd_Amt',\n",
      "       'Avg_Mdcr_Pymt_Amt', 'Avg_Mdcr_Stdzd_Amt'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "medicare_df = pd.read_csv(medicare_path, low_memory=False)\n",
    "print(f\"  Medicare: {len(medicare_df):,} rows, {len(medicare_df.columns)} columns\")\n",
    "print(medicare_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e577b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [\n",
    "    \"Covered_Recipient_Profile_ID\",  # Physician Profile ID\n",
    "    \"Covered_Recipient_First_Name\",\n",
    "    \"Covered_Recipient_Last_Name\",\n",
    "    \"Recipient_Primary_Business_Street_Address_Line1\",\n",
    "    \"Recipient_City\",\n",
    "    \"Recipient_State\",\n",
    "    \"Recipient_Zip_Code\",\n",
    "    \"Covered_Recipient_NPI\",  # may be missing\n",
    "    \"Total_Amount_of_Payment_USDollars\",\n",
    "    \"Date_of_Payment\",\n",
    "    \"Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name\",\n",
    "    \"Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name\",\n",
    "    \"Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_State\",\n",
    "    \"Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Country\",\n",
    "    \"Program_Year\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0866ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_11512\\2386880916.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: 14,700,786 rows, 15 columns\n"
     ]
    }
   ],
   "source": [
    "chunk_list = []\n",
    "for chunk in pd.read_csv(open_payments_path, usecols=use_cols, chunksize=200_000):\n",
    "    # Optional: filter, clean, or sample chunk here\n",
    "    chunk_list.append(chunk)\n",
    "\n",
    "# Concatenate after all chunks are collected\n",
    "open_payments_df = pd.concat(chunk_list, ignore_index=True)\n",
    "print(f\"Loaded dataset: {len(open_payments_df):,} rows, {len(open_payments_df.columns)} columns\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1166513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 setup complete.\n",
      "PECOS shape:         (2936748, 11)\n",
      "Medicare shape:      (9660647, 28)\n",
      "Open Payments shape: (14700786, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"Phase 2 setup complete.\")\n",
    "print(f\"PECOS shape:         {pecos_df.shape}\")\n",
    "print(f\"Medicare shape:      {medicare_df.shape}\")\n",
    "print(f\"Open Payments shape: {open_payments_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe9ee2",
   "metadata": {},
   "source": [
    "## 2.1 Importing the Preprocessing Module\n",
    "\n",
    "This section connects the notebook to a reusable Python module (`preprocessing.py`) that implements the core cleaning and validation functions used throughout Phase 2.\n",
    "\n",
    "- **Module location**\n",
    "  - The module is stored in a dedicated `lib/` folder (separate from notebooks) to enforce a clean project structure and support reuse across phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b40a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, json, pandas as pd\n",
    "\n",
    "OUTPUT_DIR = \"../artifacts/phase2_preprocessing\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# pecos_df, medicare_df, open_payments_df already loaded from Phase 1,\n",
    "# or load them here from raw files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27b55615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spot check 1053656744: True\n"
     ]
    }
   ],
   "source": [
    "# 2.1 IMPORT PREPROCESSING MODULE\n",
    "LIB_DIR = \"../lib\"   # adjust if needed\n",
    "if LIB_DIR not in sys.path:\n",
    "    sys.path.insert(0, LIB_DIR)\n",
    "\n",
    "import importlib, preprocessing\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "from preprocessing import (\n",
    "    clean_name, soundex_code, metaphone_code,\n",
    "    clean_street, clean_city, clean_state,\n",
    "    normalize_zip5, is_valid_npi,\n",
    ")\n",
    "\n",
    "print(\"Spot check 1053656744:\", is_valid_npi(1053656744))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772ad500",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../artifacts/phase2_preprocessing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976fbc86",
   "metadata": {},
   "source": [
    "### 2.2 PECOS – Enrollment & Identity Standardization\n",
    "\n",
    "In this section we transform the PECOS enrollment file into a cleaner, feature-enriched provider registry suitable for linkage and temporal analysis.\n",
    "\n",
    "- **Enrollment ID decoding**\n",
    "  - Parse `ENRLMT_ID` into its logical components:\n",
    "    - `ENRLMT_ENTITY`: first character (`I` = Individual, `O` = Organization).\n",
    "    - `ENRLMT_DATE`: enrollment date derived from the `YYYYMMDD` segment.\n",
    "    - `ENRLMT_YEAR`: enrollment calendar year for easier temporal filtering.\n",
    "    - `ENRLMT_SEQ`: trailing sequence number, preserved as an identifier.\n",
    "  - Confirm that all derived dates are valid and inspect the full date range (2002–2025), reinforcing that PECOS is a cumulative registry.\n",
    "\n",
    "- **Name standardization**\n",
    "  - Clean `FIRST_NAME`, `MDL_NAME`, `LAST_NAME`, and `ORG_NAME`:\n",
    "    - Convert to uppercase, strip whitespace, collapse multiple spaces, and remove trailing punctuation.\n",
    "  - This reduces variation caused by formatting differences and prepares names for phonetic encoding and fuzzy matching.\n",
    "\n",
    "- **Phonetic features for individuals**\n",
    "  - For records where `ENRLMT_ENTITY = 'I'`, compute:\n",
    "    - `FIRST_NAME_SOUNDEX`, `LAST_NAME_SOUNDEX`.\n",
    "    - `FIRST_NAME_METAPHONE`, `LAST_NAME_METAPHONE`.\n",
    "  - These fields will later support robust matching in dirty-data scenarios (Scenario 2) when exact text matches are unreliable.\n",
    "\n",
    "- **NPI validation**\n",
    "  - Apply the NPI checksum (Luhn with the `80840` prefix) to each `NPI`.\n",
    "  - After correcting the parity logic, 100% of PECOS NPIs pass validation, and `NPI_VALID` is set to `True` for all records.\n",
    "\n",
    "- **Name presence sanity check**\n",
    "  - Verify that every PECOS record has at least one of `FIRST_NAME`, `LAST_NAME`, or `ORG_NAME` populated.\n",
    "  - Confirm that there are zero records with all three name fields missing, so no PECOS rows need to be dropped on this basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f32ec59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 PECOS PREPROCESSING\n",
      "------------------------------------------------------------\n",
      "Enrollment date range: 2002-08-01 00:00:00 → 2025-09-13 00:00:00\n",
      "Invalid ENRLMT_DATE rows: 0\n",
      "Invalid NPIs in PECOS: 0\n",
      "Records with no FIRST/LAST/ORG name: 0\n"
     ]
    }
   ],
   "source": [
    "# 2.2 PECOS PREPROCESSING\n",
    "\n",
    "print(\"2.2 PECOS PREPROCESSING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "pecos = pecos_df.copy()\n",
    "\n",
    "# 1) ENRLMT_ID → entity, date, sequence (already mostly done in EDA, but ensure here)\n",
    "pecos['ENRLMT_ENTITY'] = pecos['ENRLMT_ID'].str[0]\n",
    "pecos['ENRLMT_DATE'] = pd.to_datetime(pecos['ENRLMT_ID'].str[1:9], format='%Y%m%d', errors='coerce')\n",
    "pecos['ENRLMT_YEAR'] = pecos['ENRLMT_DATE'].dt.year\n",
    "pecos['ENRLMT_SEQ'] = pecos['ENRLMT_ID'].str[9:]\n",
    "\n",
    "print(\"Enrollment date range:\", pecos['ENRLMT_DATE'].min(), \"→\", pecos['ENRLMT_DATE'].max())\n",
    "print(\"Invalid ENRLMT_DATE rows:\", pecos['ENRLMT_DATE'].isna().sum())\n",
    "\n",
    "# 2) Name standardization\n",
    "for col in ['FIRST_NAME', 'MDL_NAME', 'LAST_NAME', 'ORG_NAME']:\n",
    "    if col in pecos.columns:\n",
    "        pecos[col] = pecos[col].apply(clean_name)\n",
    "\n",
    "# 3) Phonetic columns for individuals (Scenario 2 support)\n",
    "is_individual = pecos['ENRLMT_ENTITY'] == 'I'\n",
    "pecos.loc[is_individual, 'FIRST_NAME_SOUNDEX'] = pecos.loc[is_individual, 'FIRST_NAME'].apply(soundex_code)\n",
    "pecos.loc[is_individual, 'LAST_NAME_SOUNDEX'] = pecos.loc[is_individual, 'LAST_NAME'].apply(soundex_code)\n",
    "pecos.loc[is_individual, 'FIRST_NAME_METAPHONE'] = pecos.loc[is_individual, 'FIRST_NAME'].apply(metaphone_code)\n",
    "pecos.loc[is_individual, 'LAST_NAME_METAPHONE'] = pecos.loc[is_individual, 'LAST_NAME'].apply(metaphone_code)\n",
    "\n",
    "# 4) NPI validation\n",
    "pecos['NPI_VALID'] = pecos['NPI'].apply(is_valid_npi)\n",
    "invalid_npi_count = (~pecos['NPI_VALID']).sum()\n",
    "print(\"Invalid NPIs in PECOS:\", invalid_npi_count)\n",
    "\n",
    "# 5) Basic sanity: records with no name fields at all\n",
    "no_name_mask = pecos['FIRST_NAME'].isna() & pecos['LAST_NAME'].isna() & pecos['ORG_NAME'].isna()\n",
    "no_name_count = no_name_mask.sum()\n",
    "print(\"Records with no FIRST/LAST/ORG name:\", no_name_count)\n",
    "\n",
    "# (Save deferred — adding Fix 2/3/6 columns first)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fix2_md",
   "metadata": {},
   "source": [
    "#### Fix 2: Flag 575 Dual-Entity NPIs\n",
    "\n",
    "From EDA Part 5, 575 NPIs appear as both Individual (`I`) and Organization (`O`) in PECOS — likely data entry errors. We flag these for cautious handling in linkage rather than dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fix2_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual-entity NPIs flagged: 1,198 rows (575 unique NPIs)\n"
     ]
    }
   ],
   "source": [
    "# Fix 2: Flag NPIs that appear as both Individual AND Organization\n",
    "entity_per_npi = pecos.groupby('NPI')['ENRLMT_ENTITY'].nunique()\n",
    "dual_npis = entity_per_npi[entity_per_npi > 1].index\n",
    "pecos['DUAL_ENTITY_FLAG'] = pecos['NPI'].isin(dual_npis)\n",
    "print(f\"Dual-entity NPIs flagged: {pecos['DUAL_ENTITY_FLAG'].sum():,} rows ({len(dual_npis)} unique NPIs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fix3_md",
   "metadata": {},
   "source": [
    "#### Fix 3: Temporal Alignment Flag\n",
    "\n",
    "Medicare and Open Payments data are both program year 2023. This flag enables Phase 3 to filter PECOS for temporal alignment without re-deriving dates from `ENRLMT_ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fix3_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrolled by 2023: 2,405,538 / 2,936,748 (81.9%)\n"
     ]
    }
   ],
   "source": [
    "# Fix 3: Temporal flag — enrolled by end of 2023 (aligns with Medicare/OP program year)\n",
    "pecos['ENROLLED_BY_2023'] = pecos['ENRLMT_YEAR'] <= 2023\n",
    "print(f\"Enrolled by 2023: {pecos['ENROLLED_BY_2023'].sum():,} / {len(pecos):,} ({pecos['ENROLLED_BY_2023'].mean()*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fix6_md",
   "metadata": {},
   "source": [
    "#### Fix 6: Multi-NPI Organization Dedup Prep\n",
    "\n",
    "From EDA Part 5, 14,210 `PECOS_ASCT_CNTL_ID` values link to multiple NPIs — these are parent organizations with multiple billing locations. We identify them and assign a canonical `PARENT_NPI` (lowest NPI per group) to support an optional organization matching pipeline in Phase 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fix6_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-NPI organizations: 14,210 parent orgs covering 114,062 rows\n",
      "PARENT_NPI assigned for org-level dedup\n"
     ]
    }
   ],
   "source": [
    "# Fix 6: Multi-NPI org dedup — identify parent organizations via PECOS_ASCT_CNTL_ID\n",
    "org_mask = pecos['ENRLMT_ENTITY'] == 'O'\n",
    "org_npi_counts = pecos.loc[org_mask].groupby('PECOS_ASCT_CNTL_ID')['NPI'].nunique()\n",
    "multi_npi_orgs = org_npi_counts[org_npi_counts > 1]\n",
    "\n",
    "pecos['MULTI_NPI_ORG'] = False\n",
    "pecos.loc[\n",
    "    org_mask & pecos['PECOS_ASCT_CNTL_ID'].isin(multi_npi_orgs.index),\n",
    "    'MULTI_NPI_ORG'\n",
    "] = True\n",
    "\n",
    "# For each multi-NPI org group, assign the lowest NPI as canonical PARENT_NPI\n",
    "canonical = pecos.loc[org_mask].groupby('PECOS_ASCT_CNTL_ID')['NPI'].min().rename('PARENT_NPI')\n",
    "pecos = pecos.merge(canonical, on='PECOS_ASCT_CNTL_ID', how='left')\n",
    "\n",
    "print(f\"Multi-NPI organizations: {len(multi_npi_orgs):,} parent orgs covering {pecos['MULTI_NPI_ORG'].sum():,} rows\")\n",
    "print(f\"PARENT_NPI assigned for org-level dedup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pecos_save_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PECOS cleaned to: ../artifacts/phase2_preprocessing\\pecos_clean.parquet\n",
      "Final PECOS shape: (2936748, 24)\n",
      "New columns: DUAL_ENTITY_FLAG, ENROLLED_BY_2023, MULTI_NPI_ORG, PARENT_NPI\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned PECOS (with Fix 2/3/6 columns included)\n",
    "pecos_output_path = os.path.join(OUTPUT_DIR, \"pecos_clean.parquet\")\n",
    "pecos.to_parquet(pecos_output_path, index=False)\n",
    "print(\"Saved PECOS cleaned to:\", pecos_output_path)\n",
    "print(\"Final PECOS shape:\", pecos.shape)\n",
    "print(f\"New columns: DUAL_ENTITY_FLAG, ENROLLED_BY_2023, MULTI_NPI_ORG, PARENT_NPI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247e07b",
   "metadata": {},
   "source": [
    "- **Result**\n",
    "  - Save the cleaned, enriched PECOS dataset to `pecos_clean.parquet`.\n",
    "  - This file now provides:\n",
    "    - Validated NPIs.\n",
    "    - Explicit entity type (individual vs organization).\n",
    "    - Enrollment dates and years.\n",
    "    - Standardized names plus phonetic variants.\n",
    "  - It serves as the primary enrollment backbone for later linkage and temporal analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cadf61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freed pecos_df. pecos working copy: (2936748, 24)\n",
      "Memory: 2.54 GB\n"
     ]
    }
   ],
   "source": [
    "# Free raw DataFrames — we've already copied what we need into `pecos`\n",
    "# medicare_df and open_payments_df will be re-copied in their own sections\n",
    "import gc\n",
    "del pecos_df\n",
    "gc.collect()\n",
    "print(f\"Freed pecos_df. pecos working copy: {pecos.shape}\")\n",
    "print(f\"Memory: {pecos.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eabcc0",
   "metadata": {},
   "source": [
    "### 2.3 Medicare – Provider-Level Aggregation\n",
    "\n",
    "The raw Medicare file is at the **service level**, where each row corresponds to a single NPI combined with a specific HCPCS code (and other service attributes). This means the same provider (same NPI) appears on many rows. In this step, we transform it into a **provider-level** dataset with one row per NPI.\n",
    "\n",
    "**Steps performed in this cell:**\n",
    "\n",
    "- **Name & credential cleaning**\n",
    "  - Standardize rendering provider names to a consistent format (uppercase, trimmed, punctuation removed).\n",
    "  - Normalize credentials (e.g., `M.D.` → `MD`) to reduce variation.\n",
    "\n",
    "- **Address standardization** (Note: `Rndrng_Prvdr_St2` dropped — 76.1% null per EDA decision matrix)\n",
    "  - Clean and normalize street, city, and state fields (uppercase, standardized abbreviations) to support downstream comparison and blocking.\n",
    "  - ZIP codes are already in clean 5‑digit format in this dataset.\n",
    "\n",
    "- **NPI validation**\n",
    "  - Apply the NPI checksum (Luhn) algorithm to flag any invalid NPIs.\n",
    "  - This ensures we only carry forward structurally valid identifiers into the linkage pipeline.\n",
    "\n",
    "- **Phonetic features for names**\n",
    "  - For individual providers, compute Soundex and Metaphone codes for first and last names.\n",
    "  - These features will be used later for robust fuzzy matching (Scenario 2).\n",
    "\n",
    "- **Provider-level deduplication (key step)**\n",
    "  - Group the data by `Rndrng_NPI` and aggregate service-level fields:\n",
    "    - `total_services` = sum of `Tot_Srvcs` across all rows for that NPI.\n",
    "    - `total_beneficiaries` = sum of `Tot_Benes`.\n",
    "    - `total_submitted_charges` = sum of (`Avg_Sbmtd_Chrg` × `Tot_Srvcs`) — weighted total, not sum of averages.\n",
    "    - `total_medicare_payment` = sum of (`Avg_Mdcr_Pymt_Amt` × `Tot_Srvcs`) — weighted total, not sum of averages.\n",
    "    - `unique_hcpcs_count` = number of distinct `HCPCS_Cd` per NPI.\n",
    "    - `service_row_count` = number of service rows for that NPI.\n",
    "  - For provider attributes that are constant per NPI (name, address, specialty, etc.), keep the first occurrence as the canonical provider record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82e21ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3 MEDICARE PREPROCESSING\n",
      "------------------------------------------------------------\n",
      "Dropped: Rndrng_Prvdr_St2 (76.1% null), Avg_Mdcr_Alowd_Amt, Avg_Mdcr_Stdzd_Amt (redundant)\n",
      "Invalid NPIs: 0\n",
      "Cleaned Medicare shape (before dedup): (9660647, 30)\n"
     ]
    }
   ],
   "source": [
    "# 2.3 MEDICARE PREPROCESSING\n",
    "\n",
    "print(\"2.3 MEDICARE PREPROCESSING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "medicare = medicare_df.copy()\n",
    "\n",
    "# 1) Name standardization\n",
    "medicare['Rndrng_Prvdr_First_Name'] = medicare['Rndrng_Prvdr_First_Name'].apply(clean_name)\n",
    "medicare['Rndrng_Prvdr_Last_Org_Name'] = medicare['Rndrng_Prvdr_Last_Org_Name'].apply(clean_name)\n",
    "medicare['Rndrng_Prvdr_MI'] = medicare['Rndrng_Prvdr_MI'].apply(clean_name)\n",
    "\n",
    "# 2) Credentials cleanup (M.D. → MD, D.O. → DO, etc.)\n",
    "def clean_credential(s):\n",
    "    if s is None or pd.isna(s):\n",
    "        return None\n",
    "    s = str(s).strip().upper()\n",
    "    s = s.replace(\".\", \"\").replace(\",\", \"\").strip()\n",
    "    return s or None\n",
    "\n",
    "medicare['Rndrng_Prvdr_Crdntls'] = medicare['Rndrng_Prvdr_Crdntls'].apply(clean_credential)\n",
    "\n",
    "# 3) Address standardization\n",
    "medicare['Rndrng_Prvdr_St1'] = medicare['Rndrng_Prvdr_St1'].apply(clean_street)\n",
    "medicare['Rndrng_Prvdr_City'] = medicare['Rndrng_Prvdr_City'].apply(clean_city)\n",
    "medicare['Rndrng_Prvdr_State_Abrvtn'] = medicare['Rndrng_Prvdr_State_Abrvtn'].apply(clean_state)\n",
    "# ZIP already 5-digit in Medicare ✓\n",
    "\n",
    "# Drop Rndrng_Prvdr_St2 — 76.1% null, too sparse for matching (per EDA Section 9 decision matrix)\n",
    "medicare.drop(columns=['Rndrng_Prvdr_St2'], inplace=True)\n",
    "\n",
    "# Drop redundant payment columns (r > 0.90 with Avg_Mdcr_Pymt_Amt per EDA Section 6)\n",
    "medicare.drop(columns=['Avg_Mdcr_Alowd_Amt', 'Avg_Mdcr_Stdzd_Amt'], inplace=True)\n",
    "print(\"Dropped: Rndrng_Prvdr_St2 (76.1% null), Avg_Mdcr_Alowd_Amt, Avg_Mdcr_Stdzd_Amt (redundant)\")\n",
    "\n",
    "# 4) NPI validation\n",
    "medicare['NPI_VALID'] = medicare['Rndrng_NPI'].apply(is_valid_npi)\n",
    "print(f\"Invalid NPIs: {(~medicare['NPI_VALID']).sum()}\")\n",
    "\n",
    "# 5) Phonetic columns (individual records only)\n",
    "is_indiv = medicare['Rndrng_Prvdr_Ent_Cd'] == 'I'\n",
    "medicare.loc[is_indiv, 'FIRST_NAME_SOUNDEX'] = medicare.loc[is_indiv, 'Rndrng_Prvdr_First_Name'].apply(soundex_code)\n",
    "medicare.loc[is_indiv, 'LAST_NAME_SOUNDEX'] = medicare.loc[is_indiv, 'Rndrng_Prvdr_Last_Org_Name'].apply(soundex_code)\n",
    "medicare.loc[is_indiv, 'FIRST_NAME_METAPHONE'] = medicare.loc[is_indiv, 'Rndrng_Prvdr_First_Name'].apply(metaphone_code)\n",
    "medicare.loc[is_indiv, 'LAST_NAME_METAPHONE'] = medicare.loc[is_indiv, 'Rndrng_Prvdr_Last_Org_Name'].apply(metaphone_code)\n",
    "\n",
    "print(f\"Cleaned Medicare shape (before dedup): {medicare.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91195523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames currently in memory:\n",
      "\n",
      "Name: medicare_df\n",
      "Shape: (9660647, 28)\n",
      "----------------------------------------\n",
      "Name: chunk\n",
      "Shape: (100786, 15)\n",
      "----------------------------------------\n",
      "Name: open_payments_df\n",
      "Shape: (14700786, 15)\n",
      "----------------------------------------\n",
      "Name: pecos\n",
      "Shape: (2936748, 24)\n",
      "----------------------------------------\n",
      "Name: medicare\n",
      "Shape: (9660647, 30)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataframes = {\n",
    "    name: obj for name, obj in globals().items()\n",
    "    if isinstance(obj, pd.DataFrame)\n",
    "}\n",
    "\n",
    "if dataframes:\n",
    "    print(\"DataFrames currently in memory:\\n\")\n",
    "    for name, df in dataframes.items():\n",
    "        print(f\"Name: {name}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"No pandas DataFrames currently in memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66f2c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medicare BEFORE dedup: 9,660,647 rows\n",
      "Medicare AFTER dedup:  1,175,281 rows\n",
      "Reduction: 87.8%\n",
      "Saved to: ../artifacts/phase2_preprocessing\\medicare_clean.parquet\n"
     ]
    }
   ],
   "source": [
    "# 6) Provider-level dedup: collapse multiple HCPCS rows per NPI → 1 row\n",
    "\n",
    "provider_cols = [\n",
    "    'Rndrng_NPI', 'Rndrng_Prvdr_Last_Org_Name', 'Rndrng_Prvdr_First_Name',\n",
    "    'Rndrng_Prvdr_MI', 'Rndrng_Prvdr_Crdntls', 'Rndrng_Prvdr_Ent_Cd',\n",
    "    'Rndrng_Prvdr_St1', 'Rndrng_Prvdr_City',\n",
    "    'Rndrng_Prvdr_State_Abrvtn', 'Rndrng_Prvdr_State_FIPS', 'Rndrng_Prvdr_Zip5',\n",
    "    'Rndrng_Prvdr_RUCA', 'Rndrng_Prvdr_RUCA_Desc', 'Rndrng_Prvdr_Cntry',\n",
    "    'Rndrng_Prvdr_Type', 'Rndrng_Prvdr_Mdcr_Prtcptg_Ind',\n",
    "    'NPI_VALID', 'FIRST_NAME_SOUNDEX', 'LAST_NAME_SOUNDEX',\n",
    "    'FIRST_NAME_METAPHONE', 'LAST_NAME_METAPHONE'\n",
    "]\n",
    "\n",
    "# Compute weighted totals (Fix 1: avg × volume, not sum of averages)\n",
    "medicare['_total_submitted_charges'] = medicare['Avg_Sbmtd_Chrg'] * medicare['Tot_Srvcs']\n",
    "medicare['_total_medicare_payment']  = medicare['Avg_Mdcr_Pymt_Amt'] * medicare['Tot_Srvcs']\n",
    "\n",
    "# Aggregate service-level stats per NPI\n",
    "service_agg = medicare.groupby('Rndrng_NPI').agg(\n",
    "    total_services=('Tot_Srvcs', 'sum'),\n",
    "    total_beneficiaries=('Tot_Benes', 'sum'),\n",
    "    total_submitted_charges=('_total_submitted_charges', 'sum'),\n",
    "    total_medicare_payment=('_total_medicare_payment', 'sum'),\n",
    "    unique_hcpcs_count=('HCPCS_Cd', 'nunique'),\n",
    "    service_row_count=('HCPCS_Cd', 'count'),\n",
    ").reset_index()\n",
    "\n",
    "# Get one row per NPI for provider-level fields\n",
    "provider_info = medicare[provider_cols].drop_duplicates(subset='Rndrng_NPI', keep='first')\n",
    "\n",
    "# Merge\n",
    "medicare_dedup = provider_info.merge(service_agg, on='Rndrng_NPI', how='left')\n",
    "\n",
    "print(f\"Medicare BEFORE dedup: {len(medicare):,} rows\")\n",
    "print(f\"Medicare AFTER dedup:  {len(medicare_dedup):,} rows\")\n",
    "print(f\"Reduction: {(1 - len(medicare_dedup)/len(medicare))*100:.1f}%\")\n",
    "\n",
    "# Save\n",
    "medicare_output = os.path.join(OUTPUT_DIR, \"medicare_clean.parquet\")\n",
    "medicare_dedup.to_parquet(medicare_output, index=False)\n",
    "print(f\"Saved to: {medicare_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce2a0d",
   "metadata": {},
   "source": [
    "- **Result**\n",
    "  - Reduce the dataset from **9,660,647 service rows** to **1,175,281 provider rows**, an **87.8% reduction** in row count.\n",
    "  - Each remaining row represents a single provider (one NPI) with:\n",
    "    - Cleaned identity and address fields.\n",
    "    - Phonetic name encodings.\n",
    "    - Aggregated utilization and payment statistics.\n",
    "\n",
    "- **Why this matters**\n",
    "  - Later phases (blocking, matching, and multi-source linkage) operate at the **provider level**, not at the HCPCS-line level.\n",
    "  - This aggregation drastically reduces computation while preserving the information needed for linkage and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58a41034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freed medicare_df. medicare working copy: (9660647, 32)\n",
      "Memory: 2.54 GB\n"
     ]
    }
   ],
   "source": [
    "# Free raw DataFrames — we've already copied what we need into `pecos`\n",
    "# medicare_df and open_payments_df will be re-copied in their own sections\n",
    "import gc\n",
    "del medicare_df\n",
    "gc.collect()\n",
    "print(f\"Freed medicare_df. medicare working copy: {medicare.shape}\")\n",
    "print(f\"Memory: {pecos.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e051af",
   "metadata": {},
   "source": [
    "### 2.4 Open Payments Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91ee88e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4 OPEN PAYMENTS PREPROCESSING\n",
      "------------------------------------------------------------\n",
      "Rows with no name AND no NPI: 31,400 — flagging as unlinkable\n",
      "ZIP+4 → ZIP5 conversion done.\n",
      "Sample ZIP5: ['55369', '46219', '98101', '89014', '43551']\n",
      "Invalid NPIs: 4\n",
      "\n",
      "Linkage tier distribution (row-level, before dedup):\n",
      "linkage_tier\n",
      "tier1_npi      14656549\n",
      "unmatchable       31424\n",
      "tier2_fuzzy       12813\n",
      "\n",
      "Cleaned Open Payments shape (before dedup): (14700786, 23)\n"
     ]
    }
   ],
   "source": [
    "# 2.4 OPEN PAYMENTS PREPROCESSING\n",
    "\n",
    "print(\"2.4 OPEN PAYMENTS PREPROCESSING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "op = open_payments_df.copy()\n",
    "\n",
    "# 1) Flag rows with no name AND no NPI (unlinkable records)\n",
    "no_identity = (\n",
    "    op['Covered_Recipient_First_Name'].isna() &\n",
    "    op['Covered_Recipient_Last_Name'].isna() &\n",
    "    op['Covered_Recipient_NPI'].isna()\n",
    ")\n",
    "print(f\"Rows with no name AND no NPI: {no_identity.sum():,} — flagging as unlinkable\")\n",
    "op['LINKABLE'] = ~no_identity\n",
    "\n",
    "# 2) NPI: float → int (handle NaN safely)\n",
    "op['Covered_Recipient_NPI'] = pd.to_numeric(op['Covered_Recipient_NPI'], errors='coerce')\n",
    "op.loc[op['Covered_Recipient_NPI'].notna(), 'Covered_Recipient_NPI'] = (\n",
    "    op.loc[op['Covered_Recipient_NPI'].notna(), 'Covered_Recipient_NPI'].astype(int)\n",
    ")\n",
    "\n",
    "# 3) Name standardization\n",
    "op['Covered_Recipient_First_Name'] = op['Covered_Recipient_First_Name'].apply(clean_name)\n",
    "op['Covered_Recipient_Last_Name']  = op['Covered_Recipient_Last_Name'].apply(clean_name)\n",
    "\n",
    "# 4) Address standardization\n",
    "op['Recipient_Primary_Business_Street_Address_Line1'] = (\n",
    "    op['Recipient_Primary_Business_Street_Address_Line1'].apply(clean_street)\n",
    ")\n",
    "op['Recipient_City']  = op['Recipient_City'].apply(clean_city)\n",
    "op['Recipient_State'] = op['Recipient_State'].apply(clean_state)\n",
    "\n",
    "# 5) ZIP → 5-digit\n",
    "op['Recipient_Zip5'] = op['Recipient_Zip_Code'].apply(normalize_zip5)\n",
    "print(\"ZIP+4 → ZIP5 conversion done.\")\n",
    "print(\"Sample ZIP5:\", op['Recipient_Zip5'].dropna().head(5).tolist())\n",
    "\n",
    "# 6) NPI validation (only where NPI exists)\n",
    "op['NPI_VALID'] = op['Covered_Recipient_NPI'].apply(\n",
    "    lambda x: is_valid_npi(int(x)) if pd.notna(x) else None\n",
    ")\n",
    "invalid_npi = op.loc[op['NPI_VALID'] == False]\n",
    "print(f\"Invalid NPIs: {len(invalid_npi):,}\")\n",
    "\n",
    "# 7) Phonetic columns (for all rows with a name)\n",
    "has_name = op['Covered_Recipient_First_Name'].notna()\n",
    "op.loc[has_name, 'FIRST_NAME_SOUNDEX']   = op.loc[has_name, 'Covered_Recipient_First_Name'].apply(soundex_code)\n",
    "op.loc[has_name, 'LAST_NAME_SOUNDEX']    = op.loc[has_name, 'Covered_Recipient_Last_Name'].apply(soundex_code)\n",
    "op.loc[has_name, 'FIRST_NAME_METAPHONE'] = op.loc[has_name, 'Covered_Recipient_First_Name'].apply(metaphone_code)\n",
    "op.loc[has_name, 'LAST_NAME_METAPHONE']  = op.loc[has_name, 'Covered_Recipient_Last_Name'].apply(metaphone_code)\n",
    "\n",
    "# 8) Assign linkage tier\n",
    "has_npi = op['NPI_VALID'] == True\n",
    "\n",
    "has_real_name_and_state = (\n",
    "    op['Covered_Recipient_First_Name'].notna() &\n",
    "    (op['Covered_Recipient_First_Name'] != 'NAN') &\n",
    "    op['Covered_Recipient_Last_Name'].notna() &\n",
    "    (op['Covered_Recipient_Last_Name'] != 'NAN') &\n",
    "    op['Recipient_State'].notna() &\n",
    "    (op['Recipient_State'] != 'NAN')\n",
    ")\n",
    "\n",
    "op['linkage_tier'] = 'unmatchable'\n",
    "op.loc[has_npi, 'linkage_tier'] = 'tier1_npi'\n",
    "op.loc[~has_npi & has_real_name_and_state, 'linkage_tier'] = 'tier2_fuzzy'\n",
    "\n",
    "print(\"\\nLinkage tier distribution (row-level, before dedup):\")\n",
    "print(op['linkage_tier'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nCleaned Open Payments shape (before dedup): {op.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccfd93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Invalid NPIs in Open Payments:\n",
      "          Covered_Recipient_NPI Covered_Recipient_First_Name Covered_Recipient_Last_Name Recipient_State\n",
      "5805590            1.202321e+09                         REZA                 FARDSHISHEH              VA\n",
      "13471051           1.356763e+09                  CHRISTOPHER                      AQUINO              FL\n",
      "14472428           1.374625e+09                       TRACEY                      TOBACK              NY\n",
      "14611723           1.851791e+09                          MEL                      IRVINE              FL\n"
     ]
    }
   ],
   "source": [
    "# Inspect the 4 invalid NPIs\n",
    "invalid_ops = op[op['NPI_VALID'] == False]\n",
    "print(\"4 Invalid NPIs in Open Payments:\")\n",
    "print(invalid_ops[['Covered_Recipient_NPI', 'Covered_Recipient_First_Name', \n",
    "                    'Covered_Recipient_Last_Name', 'Recipient_State']].drop_duplicates().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7babed7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open Payments BEFORE dedup: 14,700,786 rows\n",
      "Open Payments AFTER dedup:  969,703 rows\n",
      "Reduction: 93.4%\n",
      "Linkage tier distribution:\n",
      "linkage_tier\n",
      "tier1_npi      933615\n",
      "unmatchable     31405\n",
      "tier2_fuzzy      4683\n",
      "\n",
      "Saved to: ../artifacts/phase2_preprocessing\\open_payments_clean.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open Payments: Provider-level dedup + save\n",
    "import gc\n",
    "\n",
    "\n",
    "# Aggregate to provider level (one row per NPI/Profile)\n",
    "op_provider_cols = [\n",
    "    'Covered_Recipient_NPI', 'Covered_Recipient_Profile_ID',\n",
    "    'Covered_Recipient_First_Name', 'Covered_Recipient_Last_Name',\n",
    "    'Recipient_Primary_Business_Street_Address_Line1',\n",
    "    'Recipient_City', 'Recipient_State', 'Recipient_Zip5',\n",
    "    'NPI_VALID', 'LINKABLE', 'linkage_tier',\n",
    "    'FIRST_NAME_SOUNDEX', 'LAST_NAME_SOUNDEX',\n",
    "    'FIRST_NAME_METAPHONE', 'LAST_NAME_METAPHONE'\n",
    "]\n",
    "\n",
    "\n",
    "# Group by NPI (or Profile ID for NPI-missing rows)\n",
    "op['Date_of_Payment'] = pd.to_datetime(op['Date_of_Payment'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "\n",
    "# Use NPI as primary key; Profile_ID as fallback; row index as last resort\n",
    "npi_str = op.loc[op['Covered_Recipient_NPI'].notna(), 'Covered_Recipient_NPI'].astype(int).astype(str)\n",
    "pid_valid = op['Covered_Recipient_Profile_ID'].notna()\n",
    "\n",
    "op['_group_key'] = 'ROW_' + op.index.astype(str)                                     # last resort: unique per row\n",
    "op.loc[pid_valid, '_group_key'] = 'PID_' + op.loc[pid_valid, 'Covered_Recipient_Profile_ID'].astype(str)\n",
    "op.loc[op['Covered_Recipient_NPI'].notna(), '_group_key'] = 'NPI_' + npi_str\n",
    "\n",
    "\n",
    "payment_agg = op.groupby('_group_key').agg(\n",
    "    total_payment_amount=('Total_Amount_of_Payment_USDollars', 'sum'),\n",
    "    payment_count=('Total_Amount_of_Payment_USDollars', 'count'),\n",
    "    avg_payment=('Total_Amount_of_Payment_USDollars', 'mean'),\n",
    "    max_payment=('Total_Amount_of_Payment_USDollars', 'max'),\n",
    "    unique_manufacturers=('Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name', 'nunique'),\n",
    "    min_payment_date=('Date_of_Payment', 'min'),\n",
    "    max_payment_date=('Date_of_Payment', 'max'),\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "provider_info = op[op_provider_cols + ['_group_key']].drop_duplicates(\n",
    "    subset='_group_key', keep='first'\n",
    ")\n",
    "\n",
    "\n",
    "op_dedup = provider_info.merge(payment_agg, on='_group_key', how='left')\n",
    "op_dedup.drop(columns=['_group_key'], inplace=True)\n",
    "\n",
    "\n",
    "print(f\"Open Payments BEFORE dedup: {len(op):,} rows\")\n",
    "print(f\"Open Payments AFTER dedup:  {len(op_dedup):,} rows\")\n",
    "print(f\"Reduction: {(1 - len(op_dedup)/len(op))*100:.1f}%\")\n",
    "print(f\"Linkage tier distribution:\")\n",
    "print(op_dedup['linkage_tier'].value_counts().to_string())\n",
    "\n",
    "\n",
    "# Save\n",
    "op_output = os.path.join(OUTPUT_DIR, \"open_payments_clean.parquet\")\n",
    "op_dedup.to_parquet(op_output, index=False)\n",
    "print(f\"\\nSaved to: {op_output}\")\n",
    "\n",
    "\n",
    "# Free memory\n",
    "del op, op_dedup, payment_agg, provider_info\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd26ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../artifacts/phase2_preprocessing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eb35816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  medicare_clean.parquet                                             78.0 MB\n",
      "  open_payments_clean.parquet                                        61.2 MB\n",
      "  pecos_clean.parquet                                               117.2 MB\n",
      "  preprocessing_report.json                                           0.0 MB\n",
      "\n",
      "--- Artifacts so far ---\n",
      "  medicare_clean.parquet                                             78.0 MB\n",
      "  open_payments_clean.parquet                                        61.2 MB\n",
      "  pecos_clean.parquet                                               117.2 MB\n",
      "  preprocessing_report.json                                           0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check what raw files exist\n",
    "for f in sorted(os.listdir(INPUT_DIR)):\n",
    "    size_mb = os.path.getsize(os.path.join(INPUT_DIR, f)) / 1e6\n",
    "    print(f\"  {f:60s} {size_mb:>10.1f} MB\")\n",
    "\n",
    "print(\"\\n--- Artifacts so far ---\")\n",
    "OUTPUT_DIR = '../artifacts/phase2_preprocessing'\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "        size_mb = os.path.getsize(os.path.join(OUTPUT_DIR, f)) / 1e6\n",
    "        print(f\"  {f:60s} {size_mb:>10.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b0a03b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier2 total: 4,683\n",
      "  Name = 'NAN': 0\n",
      "  Real names:   4,683\n",
      "\n",
      "Unmatchable count: 31,405\n"
     ]
    }
   ],
   "source": [
    "op_check = pd.read_parquet(os.path.join(OUTPUT_DIR, 'open_payments_clean.parquet'))\n",
    "\n",
    "t2 = op_check[op_check['linkage_tier'] == 'tier2_fuzzy']\n",
    "print(f\"Tier2 total: {len(t2):,}\")\n",
    "print(f\"  Name = 'NAN': {(t2['Covered_Recipient_First_Name'] == 'NAN').sum():,}\")\n",
    "print(f\"  Real names:   {(t2['Covered_Recipient_First_Name'] != 'NAN').sum():,}\")\n",
    "\n",
    "print(f\"\\nUnmatchable count: {(op_check['linkage_tier'] == 'unmatchable').sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02aaad5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample first names:\n",
      "Covered_Recipient_First_Name\n",
      "JENNIFER    61\n",
      "MICHAEL     51\n",
      "DAVID       50\n",
      "JOHN        49\n",
      "ROBERT      38\n",
      "JESSICA     38\n",
      "AMY         38\n",
      "MARY        35\n",
      "RICHARD     33\n",
      "KAREN       33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Empty string count: 0\n",
      "Whitespace-only count: 0\n",
      "\n",
      "Total tier2 rows: 4,683\n"
     ]
    }
   ],
   "source": [
    "# Check if names are real or empty strings\n",
    "t2 = op_check[op_check['linkage_tier'] == 'tier2_fuzzy']\n",
    "print(\"Sample first names:\")\n",
    "print(t2['Covered_Recipient_First_Name'].value_counts().head(10))\n",
    "print(f\"\\nEmpty string count: {(t2['Covered_Recipient_First_Name'] == '').sum():,}\")\n",
    "print(f\"Whitespace-only count: {(t2['Covered_Recipient_First_Name'].str.strip() == '').sum():,}\")\n",
    "print(f\"\\nTotal tier2 rows: {len(t2):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fix7_md",
   "metadata": {},
   "source": [
    "## 2.5 Post-Preprocessing Validation Report\n",
    "\n",
    "This section loads all three cleaned parquet files from disk and generates a validation report. It catches schema drift between phases and serves as a checkpoint before Phase 3 blocking/matching.\n",
    "\n",
    "The report includes shape, dtypes, null counts, cardinality of key columns, and file sizes — saved as `preprocessing_report.json` for programmatic access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fix7_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "2.5 POST-PREPROCESSING VALIDATION REPORT\n",
      "======================================================================\n",
      "\n",
      "--- PECOS ---\n",
      "  Shape: (2936748, 24)\n",
      "  File size: 117.18 MB\n",
      "  Columns: ['NPI', 'MULTIPLE_NPI_FLAG', 'PECOS_ASCT_CNTL_ID', 'ENRLMT_ID', 'PROVIDER_TYPE_CD', 'PROVIDER_TYPE_DESC', 'STATE_CD', 'FIRST_NAME', 'MDL_NAME', 'LAST_NAME', 'ORG_NAME', 'ENRLMT_ENTITY', 'ENRLMT_DATE', 'ENRLMT_YEAR', 'ENRLMT_SEQ', 'FIRST_NAME_SOUNDEX', 'LAST_NAME_SOUNDEX', 'FIRST_NAME_METAPHONE', 'LAST_NAME_METAPHONE', 'NPI_VALID', 'DUAL_ENTITY_FLAG', 'ENROLLED_BY_2023', 'MULTI_NPI_ORG', 'PARENT_NPI']\n",
      "  Nulls per column:\n",
      "    MDL_NAME                                                     10\n",
      "    FIRST_NAME_SOUNDEX                                      434,372\n",
      "    LAST_NAME_SOUNDEX                                       434,372\n",
      "    FIRST_NAME_METAPHONE                                    434,372\n",
      "    LAST_NAME_METAPHONE                                     434,372\n",
      "    PARENT_NPI                                            2,502,376\n",
      "  Cardinality (key columns):\n",
      "    NPI                                                   2,521,536\n",
      "    ENRLMT_ENTITY                                                 2\n",
      "    STATE_CD                                                     56\n",
      "    DUAL_ENTITY_FLAG                                              2\n",
      "    ENROLLED_BY_2023                                              2\n",
      "    MULTI_NPI_ORG                                                 2\n",
      "\n",
      "--- MEDICARE ---\n",
      "  Shape: (1175281, 27)\n",
      "  File size: 78.03 MB\n",
      "  Columns: ['Rndrng_NPI', 'Rndrng_Prvdr_Last_Org_Name', 'Rndrng_Prvdr_First_Name', 'Rndrng_Prvdr_MI', 'Rndrng_Prvdr_Crdntls', 'Rndrng_Prvdr_Ent_Cd', 'Rndrng_Prvdr_St1', 'Rndrng_Prvdr_City', 'Rndrng_Prvdr_State_Abrvtn', 'Rndrng_Prvdr_State_FIPS', 'Rndrng_Prvdr_Zip5', 'Rndrng_Prvdr_RUCA', 'Rndrng_Prvdr_RUCA_Desc', 'Rndrng_Prvdr_Cntry', 'Rndrng_Prvdr_Type', 'Rndrng_Prvdr_Mdcr_Prtcptg_Ind', 'NPI_VALID', 'FIRST_NAME_SOUNDEX', 'LAST_NAME_SOUNDEX', 'FIRST_NAME_METAPHONE', 'LAST_NAME_METAPHONE', 'total_services', 'total_beneficiaries', 'total_submitted_charges', 'total_medicare_payment', 'unique_hcpcs_count', 'service_row_count']\n",
      "  Nulls per column:\n",
      "    Rndrng_Prvdr_MI                                              18\n",
      "    Rndrng_Prvdr_Crdntls                                    153,337\n",
      "    Rndrng_Prvdr_State_FIPS                                       1\n",
      "    Rndrng_Prvdr_Zip5                                             1\n",
      "    Rndrng_Prvdr_RUCA                                         1,021\n",
      "    Rndrng_Prvdr_RUCA_Desc                                    1,021\n",
      "    FIRST_NAME_SOUNDEX                                       61,864\n",
      "    LAST_NAME_SOUNDEX                                        61,864\n",
      "    FIRST_NAME_METAPHONE                                     61,864\n",
      "    LAST_NAME_METAPHONE                                      61,864\n",
      "  Cardinality (key columns):\n",
      "    Rndrng_NPI                                            1,175,281\n",
      "    Rndrng_Prvdr_Ent_Cd                                           2\n",
      "    Rndrng_Prvdr_State_Abrvtn                                    62\n",
      "    NPI_VALID                                                     1\n",
      "\n",
      "--- OPEN_PAYMENTS ---\n",
      "  Shape: (969703, 22)\n",
      "  File size: 61.23 MB\n",
      "  Columns: ['Covered_Recipient_NPI', 'Covered_Recipient_Profile_ID', 'Covered_Recipient_First_Name', 'Covered_Recipient_Last_Name', 'Recipient_Primary_Business_Street_Address_Line1', 'Recipient_City', 'Recipient_State', 'Recipient_Zip5', 'NPI_VALID', 'LINKABLE', 'linkage_tier', 'FIRST_NAME_SOUNDEX', 'LAST_NAME_SOUNDEX', 'FIRST_NAME_METAPHONE', 'LAST_NAME_METAPHONE', 'total_payment_amount', 'payment_count', 'avg_payment', 'max_payment', 'unique_manufacturers', 'min_payment_date', 'max_payment_date']\n",
      "  Nulls per column:\n",
      "    Covered_Recipient_NPI                                    36,084\n",
      "    Covered_Recipient_Profile_ID                             31,400\n",
      "    Recipient_Zip5                                           10,783\n",
      "    NPI_VALID                                                36,084\n",
      "  Cardinality (key columns):\n",
      "    Covered_Recipient_NPI                                   933,619\n",
      "    linkage_tier                                                  3\n",
      "    Recipient_State                                              59\n",
      "    NPI_VALID                                                     2\n",
      "\n",
      "Saved: ../artifacts/phase2_preprocessing\\preprocessing_report.json\n",
      "\n",
      "======================================================================\n",
      "Dataset                      Rows   Cols  Size (MB)     Key NPIs\n",
      "------------------------------------------------------------\n",
      "pecos                   2,936,748     24      117.2    2,521,536\n",
      "medicare                1,175,281     27       78.0    1,175,281\n",
      "open_payments             969,703     22       61.2      933,619\n",
      "\n",
      "✓ Phase 2 validation complete.\n"
     ]
    }
   ],
   "source": [
    "# 2.5 POST-PREPROCESSING VALIDATION REPORT\n",
    "\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "OUTPUT_DIR = \"../artifacts/phase2_preprocessing\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"2.5 POST-PREPROCESSING VALIDATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load all three parquet files\n",
    "pecos_v = pd.read_parquet(os.path.join(OUTPUT_DIR, \"pecos_clean.parquet\"))\n",
    "medicare_v = pd.read_parquet(os.path.join(OUTPUT_DIR, \"medicare_clean.parquet\"))\n",
    "op_v = pd.read_parquet(os.path.join(OUTPUT_DIR, \"open_payments_clean.parquet\"))\n",
    "\n",
    "report = {\n",
    "    \"generated_at\": datetime.now().isoformat(),\n",
    "    \"datasets\": {}\n",
    "}\n",
    "\n",
    "# Key columns for cardinality checks\n",
    "key_cols = {\n",
    "    \"pecos\": [\"NPI\", \"ENRLMT_ENTITY\", \"STATE_CD\", \"DUAL_ENTITY_FLAG\", \"ENROLLED_BY_2023\", \"MULTI_NPI_ORG\"],\n",
    "    \"medicare\": [\"Rndrng_NPI\", \"Rndrng_Prvdr_Ent_Cd\", \"Rndrng_Prvdr_State_Abrvtn\", \"NPI_VALID\"],\n",
    "    \"open_payments\": [\"Covered_Recipient_NPI\", \"linkage_tier\", \"Recipient_State\", \"NPI_VALID\"]\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"pecos\": pecos_v,\n",
    "    \"medicare\": medicare_v,\n",
    "    \"open_payments\": op_v\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    filepath = os.path.join(OUTPUT_DIR, f\"{name.replace('open_payments', 'open_payments')}_clean.parquet\")\n",
    "    file_size_mb = round(os.path.getsize(filepath) / 1e6, 2)\n",
    "    \n",
    "    nulls = {col: int(df[col].isna().sum()) for col in df.columns}\n",
    "    dtypes = {col: str(df[col].dtype) for col in df.columns}\n",
    "    cardinality = {}\n",
    "    for col in key_cols.get(name, []):\n",
    "        if col in df.columns:\n",
    "            cardinality[col] = int(df[col].nunique())\n",
    "    \n",
    "    report[\"datasets\"][name] = {\n",
    "        \"rows\": len(df),\n",
    "        \"cols\": len(df.columns),\n",
    "        \"columns\": list(df.columns),\n",
    "        \"dtypes\": dtypes,\n",
    "        \"nulls\": nulls,\n",
    "        \"cardinality\": cardinality,\n",
    "        \"file_size_mb\": file_size_mb\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- {name.upper()} ---\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  File size: {file_size_mb} MB\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Nulls per column:\")\n",
    "    for col, n in nulls.items():\n",
    "        if n > 0:\n",
    "            print(f\"    {col:50s} {n:>12,}\")\n",
    "    print(f\"  Cardinality (key columns):\")\n",
    "    for col, c in cardinality.items():\n",
    "        print(f\"    {col:50s} {c:>12,}\")\n",
    "\n",
    "# Save JSON report\n",
    "report_path = os.path.join(OUTPUT_DIR, \"preprocessing_report.json\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "print(f\"\\nSaved: {report_path}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"{'Dataset':20s} {'Rows':>12s} {'Cols':>6s} {'Size (MB)':>10s} {'Key NPIs':>12s}\")\n",
    "print(\"-\" * 60)\n",
    "for name in [\"pecos\", \"medicare\", \"open_payments\"]:\n",
    "    d = report[\"datasets\"][name]\n",
    "    npi_col = {\"pecos\": \"NPI\", \"medicare\": \"Rndrng_NPI\", \"open_payments\": \"Covered_Recipient_NPI\"}[name]\n",
    "    npi_count = d[\"cardinality\"].get(npi_col, \"N/A\")\n",
    "    print(f\"{name:20s} {d['rows']:>12,} {d['cols']:>6d} {d['file_size_mb']:>10.1f} {npi_count:>12,}\")\n",
    "\n",
    "# Cleanup\n",
    "del pecos_v, medicare_v, op_v\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\n✓ Phase 2 validation complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
