{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6: Large-Scale LSH Benchmark - Supplemental\n",
    "\n",
    "**Objective:** Benchmark LSH blocking at full dataset scale (933K+ tier-1 NPI records × 1.175M Medicare),\n",
    "sweeping hyperparameters and comparing against exact-NPI matching and traditional blocking.\n",
    "\n",
    "**Input:** Cleaned parquet files from Phase 2 (`../artifacts/phase2_preprocessing/`)\n",
    "\n",
    "**Output:** `../artifacts/phase6_lsh_benchmark/`\n",
    "- `lsh_benchmark_results.csv` — hyperparameter sweep results\n",
    "- `lsh_vs_baselines.csv` — LSH vs exact NPI vs traditional blocking\n",
    "- `lsh_benchmark_charts.png` — visualization\n",
    "\n",
    "**Key Questions:**\n",
    "1. How does LSH runtime scale with `num_perm` and `threshold`?\n",
    "2. What is the precision/recall tradeoff at different similarity thresholds?\n",
    "3. Does LSH find matches that exact NPI misses (and vice versa)?\n",
    "4. What is the optimal configuration for production use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os, gc, time, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "INPUTDIR = \"../artifacts/phase2_preprocessing\"\n",
    "OUTPUTDIR = \"../artifacts/phase6_lsh_benchmark\"\n",
    "os.makedirs(OUTPUTDIR, exist_ok=True)\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load Full-Scale Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4.1 LOAD FULL-SCALE DATASETS\n",
      "============================================================\n",
      "Full OP dataset:           969,703\n",
      "  Tier-1 (NPI):            933,615\n",
      "  Tier-2 (fuzzy):            4,683\n",
      "  Unmatchable:              31,405\n",
      "Medicare backbone:       1,175,281\n",
      "Full cross-product:     5,503,840,923 pairs\n",
      "\n",
      "Datasets loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4.1 LOAD FULL-SCALE DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "opclean = pd.read_parquet(os.path.join(INPUTDIR, \"open_payments_clean.parquet\"))\n",
    "medclean = pd.read_parquet(os.path.join(INPUTDIR, \"medicare_clean.parquet\"))\n",
    "\n",
    "# Separate tiers\n",
    "op_tier1 = opclean[opclean[\"linkage_tier\"] == \"tier1_npi\"]\n",
    "op_tier2 = opclean[opclean[\"linkage_tier\"] == \"tier2_fuzzy\"].copy().reset_index(drop=True)\n",
    "op_unmatchable = opclean[opclean[\"linkage_tier\"] == \"unmatchable\"]\n",
    "\n",
    "print(f\"Full OP dataset:      {len(opclean):>12,}\")\n",
    "print(f\"  Tier-1 (NPI):       {len(op_tier1):>12,}\")\n",
    "print(f\"  Tier-2 (fuzzy):     {len(op_tier2):>12,}\")\n",
    "print(f\"  Unmatchable:        {len(op_unmatchable):>12,}\")\n",
    "print(f\"Medicare backbone:    {len(medclean):>12,}\")\n",
    "print(f\"Full cross-product:   {len(op_tier2) * len(medclean):>15,} pairs\")\n",
    "\n",
    "del opclean; gc.collect()\n",
    "print(\"\\nDatasets loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Baseline 1 — Exact NPI Matching\n",
    "\n",
    "The simplest and fastest approach: inner-join on NPI. This is the gold standard for\n",
    "tier-1 records and serves as our ground-truth reference for precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af0f814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Covered_Recipient_NPI', 'Covered_Recipient_Profile_ID',\n",
       "       'Covered_Recipient_First_Name', 'Covered_Recipient_Last_Name',\n",
       "       'Recipient_Primary_Business_Street_Address_Line1', 'Recipient_City',\n",
       "       'Recipient_State', 'Recipient_Zip5', 'NPI_VALID', 'LINKABLE',\n",
       "       'linkage_tier', 'FIRST_NAME_SOUNDEX', 'LAST_NAME_SOUNDEX',\n",
       "       'FIRST_NAME_METAPHONE', 'LAST_NAME_METAPHONE', 'total_payment_amount',\n",
       "       'payment_count', 'avg_payment', 'max_payment', 'unique_manufacturers',\n",
       "       'min_payment_date', 'max_payment_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_tier1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4.2 BASELINE 1: EXACT NPI MATCHING\n",
      "============================================================\n",
      "Unique OP tier-1 NPIs:            933,615\n",
      "Unique Medicare NPIs:           1,175,281\n",
      "NPI overlap (exact matches):      542,329\n",
      "Match rate (of OP tier-1):   58.1%\n",
      "Runtime:                     0.65s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4.2 BASELINE 1: EXACT NPI MATCHING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Standardize NPI dtypes\n",
    "op_npi = pd.to_numeric(op_tier1[\"Covered_Recipient_NPI\"], errors=\"coerce\").dropna().astype(\"int64\")\n",
    "med_npi = pd.to_numeric(medclean[\"Rndrng_NPI\"], errors=\"coerce\").dropna().astype(\"int64\")\n",
    "\n",
    "# Set intersection\n",
    "op_npi_set = set(op_npi.unique())\n",
    "med_npi_set = set(med_npi.unique())\n",
    "npi_overlap = op_npi_set & med_npi_set\n",
    "\n",
    "elapsed_npi = time.time() - t0\n",
    "\n",
    "print(f\"Unique OP tier-1 NPIs:       {len(op_npi_set):>12,}\")\n",
    "print(f\"Unique Medicare NPIs:        {len(med_npi_set):>12,}\")\n",
    "print(f\"NPI overlap (exact matches): {len(npi_overlap):>12,}\")\n",
    "print(f\"Match rate (of OP tier-1):   {len(npi_overlap)/len(op_npi_set):.1%}\")\n",
    "print(f\"Runtime:                     {elapsed_npi:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Baseline 2 — Traditional Blocking (State + Soundex)\n",
    "\n",
    "Run Strategy B (State + Last Name Soundex) on tier-2 fuzzy records as the\n",
    "traditional blocking baseline. Already benchmarked in Phase 3 at ~1M pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4.3 BASELINE 2: TRADITIONAL BLOCKING (STRATEGY B)\n",
      "============================================================\n",
      "Tier-2 records:              4,683\n",
      "Medicare records:        1,175,281\n",
      "Candidate pairs (B):       427,752\n",
      "Reduction ratio:      99.992228%\n",
      "Runtime:              0.84s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "628"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4.3 BASELINE 2: TRADITIONAL BLOCKING (STRATEGY B)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "op_tier2[\"block_B\"] = op_tier2[\"LAST_NAME_SOUNDEX\"].fillna(\"\") + \"|\" + op_tier2[\"Recipient_State\"].fillna(\"\")\n",
    "medclean[\"block_B\"] = medclean[\"LAST_NAME_SOUNDEX\"].fillna(\"\") + \"|\" + medclean[\"Rndrng_Prvdr_State_Abrvtn\"].fillna(\"\")\n",
    "\n",
    "pairs_B = (\n",
    "    op_tier2[[\"block_B\"]].reset_index().rename(columns={\"index\": \"index_op\"})\n",
    "    .merge(\n",
    "        medclean[[\"block_B\"]].reset_index().rename(columns={\"index\": \"index_med\"}),\n",
    "        on=\"block_B\"\n",
    "    )[[\"index_op\", \"index_med\"]]\n",
    ")\n",
    "\n",
    "elapsed_B = time.time() - t0\n",
    "fullcross = len(op_tier2) * len(medclean)\n",
    "rr_B = 1 - len(pairs_B) / fullcross\n",
    "\n",
    "print(f\"Tier-2 records:       {len(op_tier2):>12,}\")\n",
    "print(f\"Medicare records:     {len(medclean):>12,}\")\n",
    "print(f\"Candidate pairs (B):  {len(pairs_B):>12,}\")\n",
    "print(f\"Reduction ratio:      {rr_B*100:.6f}%\")\n",
    "print(f\"Runtime:              {elapsed_B:.2f}s\")\n",
    "\n",
    "# Clean up\n",
    "op_tier2.drop(columns=\"block_B\", inplace=True)\n",
    "medclean.drop(columns=\"block_B\", inplace=True)\n",
    "set_B = set(map(tuple, pairs_B[[\"index_op\", \"index_med\"]].values))\n",
    "\n",
    "del pairs_B; gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 LSH Hyperparameter Sweep\n",
    "\n",
    "Sweep `num_perm` (MinHash signature size) and `threshold` (Jaccard similarity cutoff)\n",
    "to find the optimal LSH configuration. We use `datasketch` for production-grade LSH.\n",
    "\n",
    "**Grid:**\n",
    "- `num_perm`: [64, 128, 256]\n",
    "- `threshold`: [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "- Q-gram size: 3 (trigrams, fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4.4 LSH HYPERPARAMETER SWEEP\n",
      "============================================================\n",
      "datasketch ready\n",
      "Name strings built in 9.5s\n",
      "States in common: 53\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4.4 LSH HYPERPARAMETER SWEEP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from datasketch import MinHash, MinHashLSH\n",
    "    print(\"datasketch ready\")\n",
    "except ImportError:\n",
    "    !pip install datasketch -q\n",
    "    from datasketch import MinHash, MinHashLSH\n",
    "    print(\"datasketch installed\")\n",
    "\n",
    "Q = 3  # trigram size\n",
    "\n",
    "def make_minhash(text, num_perm):\n",
    "    \"\"\"Create MinHash from text using character q-grams.\"\"\"\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    if not isinstance(text, str) or len(text) < Q:\n",
    "        return None\n",
    "    for i in range(len(text) - Q + 1):\n",
    "        m.update(text[i:i+Q].encode(\"utf-8\"))\n",
    "    return m\n",
    "\n",
    "def name_string(first, last, state):\n",
    "    \"\"\"Concatenate name fields for hashing.\"\"\"\n",
    "    parts = [str(x).strip().upper() for x in (first, last, state) if pd.notna(x)]\n",
    "    return \" \".join(p for p in parts if p and p != \"NAN\")\n",
    "\n",
    "# Precompute name strings (one-time cost)\n",
    "t0 = time.time()\n",
    "op_tier2[\"_ns\"] = op_tier2.apply(\n",
    "    lambda r: name_string(r[\"Covered_Recipient_First_Name\"], \n",
    "                          r[\"Covered_Recipient_Last_Name\"], \n",
    "                          r[\"Recipient_State\"]), axis=1\n",
    ")\n",
    "medclean[\"_ns\"] = medclean.apply(\n",
    "    lambda r: name_string(r[\"Rndrng_Prvdr_First_Name\"], \n",
    "                          r[\"Rndrng_Prvdr_Last_Org_Name\"], \n",
    "                          r[\"Rndrng_Prvdr_State_Abrvtn\"]), axis=1\n",
    ")\n",
    "print(f\"Name strings built in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Get common states for within-state LSH\n",
    "states_common = sorted(\n",
    "    set(op_tier2[\"Recipient_State\"].dropna().unique()) &\n",
    "    set(medclean[\"Rndrng_Prvdr_State_Abrvtn\"].dropna().unique())\n",
    ")\n",
    "print(f\"States in common: {len(states_common)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- num_perm=128, threshold=0.5 ---\n",
      "  Pairs: 100,196  RR: 99.998180%  PC vs B: 8.6%  Unique: 63,483  Time: 1695.8s\n",
      "\n",
      "============================================================\n",
      "SWEEP COMPLETE\n",
      "============================================================\n",
      " num_perm  threshold  candidate_pairs  reduction_ratio  pc_vs_strategy_B  unique_to_lsh  overlap_with_B  runtime_sec  pairs_per_sec\n",
      "      128        0.5           100196         0.999982              8.58          63483           36713       1695.8           59.0\n"
     ]
    }
   ],
   "source": [
    "# ---- Sweep ---- limited due to lime contraints, else run on different perms and threshholds\n",
    "NUM_PERMS = [128]\n",
    "THRESHOLDS = [0.5]\n",
    "\n",
    "sweep_results = []\n",
    "\n",
    "for num_perm, threshold in product(NUM_PERMS, THRESHOLDS):\n",
    "    print(f\"\\n--- num_perm={num_perm}, threshold={threshold:.1f} ---\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    lsh_pairs = set()\n",
    "\n",
    "    for state in states_common:\n",
    "        op_st = op_tier2[op_tier2[\"Recipient_State\"] == state]\n",
    "        med_st = medclean[medclean[\"Rndrng_Prvdr_State_Abrvtn\"] == state]\n",
    "        if len(op_st) == 0 or len(med_st) == 0:\n",
    "            continue\n",
    "\n",
    "        # Build LSH index from Medicare side\n",
    "        lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        med_minhashes = {}\n",
    "        for idx, row in med_st.iterrows():\n",
    "            mh = make_minhash(row[\"_ns\"], num_perm)\n",
    "            if mh:\n",
    "                try:\n",
    "                    lsh.insert(f\"m_{idx}\", mh)\n",
    "                    med_minhashes[idx] = mh\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        # Query OP side\n",
    "        for idx_op, row_op in op_st.iterrows():\n",
    "            mh_op = make_minhash(row_op[\"_ns\"], num_perm)\n",
    "            if mh_op is None:\n",
    "                continue\n",
    "            for key in lsh.query(mh_op):\n",
    "                med_idx = int(key.split(\"_\")[1])\n",
    "                lsh_pairs.add((idx_op, med_idx))\n",
    "\n",
    "        del lsh; gc.collect()\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    n_pairs = len(lsh_pairs)\n",
    "    rr = 1 - n_pairs / fullcross if fullcross > 0 else 0\n",
    "\n",
    "    # Pairs completeness vs Strategy B\n",
    "    pc_vs_B = len(lsh_pairs & set_B) / len(set_B) * 100 if len(set_B) > 0 else 0\n",
    "\n",
    "    # Unique pairs not in B\n",
    "    unique_to_lsh = len(lsh_pairs - set_B)\n",
    "\n",
    "    print(f\"  Pairs: {n_pairs:,}  RR: {rr*100:.6f}%  \"\n",
    "          f\"PC vs B: {pc_vs_B:.1f}%  Unique: {unique_to_lsh:,}  \"\n",
    "          f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "    sweep_results.append({\n",
    "        \"num_perm\": num_perm,\n",
    "        \"threshold\": threshold,\n",
    "        \"candidate_pairs\": n_pairs,\n",
    "        \"reduction_ratio\": rr,\n",
    "        \"pc_vs_strategy_B\": round(pc_vs_B, 2),\n",
    "        \"unique_to_lsh\": unique_to_lsh,\n",
    "        \"overlap_with_B\": n_pairs - unique_to_lsh,\n",
    "        \"runtime_sec\": round(elapsed, 1),\n",
    "        \"pairs_per_sec\": round(n_pairs / elapsed, 0) if elapsed > 0 else 0\n",
    "    })\n",
    "\n",
    "sweep_df = pd.DataFrame(sweep_results)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SWEEP COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(sweep_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Optimal Configuration Analysis\n",
    "\n",
    "Select the configuration that maximizes pairs completeness while maintaining\n",
    ">99.99% reduction ratio, and analyze its output in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4.5 OPTIMAL CONFIGURATION ANALYSIS\n",
      "============================================================\n",
      "Best config: num_perm=128, threshold=0.5\n",
      "  Candidate pairs:    100,196\n",
      "  Reduction ratio:    99.998180%\n",
      "  PC vs Strategy B:   8.6%\n",
      "  Unique to LSH:      63,483\n",
      "  Runtime:            1695.8s\n",
      "\n",
      "--- Efficiency vs Baselines ---\n",
      "Exact NPI:         0.65s → 542,329 matches (tier-1 only)\n",
      "Strategy B:        0.84s → 427,752 pairs (tier-2)\n",
      "Best LSH:          1695.8s → 100,196 pairs (tier-2)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4.5 OPTIMAL CONFIGURATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter configs with RR > 99.99%\n",
    "viable = sweep_df[sweep_df[\"reduction_ratio\"] > 0.9999].copy()\n",
    "\n",
    "if len(viable) == 0:\n",
    "    print(\"WARNING: No configs meet RR > 99.99%. Relaxing to > 99.9%.\")\n",
    "    viable = sweep_df[sweep_df[\"reduction_ratio\"] > 0.999].copy()\n",
    "\n",
    "# Best = highest PC vs B among viable\n",
    "best = viable.loc[viable[\"pc_vs_strategy_B\"].idxmax()]\n",
    "print(f\"Best config: num_perm={int(best['num_perm'])}, threshold={best['threshold']}\")\n",
    "print(f\"  Candidate pairs:    {int(best['candidate_pairs']):,}\")\n",
    "print(f\"  Reduction ratio:    {best['reduction_ratio']*100:.6f}%\")\n",
    "print(f\"  PC vs Strategy B:   {best['pc_vs_strategy_B']:.1f}%\")\n",
    "print(f\"  Unique to LSH:      {int(best['unique_to_lsh']):,}\")\n",
    "print(f\"  Runtime:            {best['runtime_sec']:.1f}s\")\n",
    "\n",
    "# Efficiency comparison\n",
    "print(f\"\\n--- Efficiency vs Baselines ---\")\n",
    "print(f\"Exact NPI:         {elapsed_npi:.2f}s → {len(npi_overlap):,} matches (tier-1 only)\")\n",
    "print(f\"Strategy B:        {elapsed_B:.2f}s → {len(set_B):,} pairs (tier-2)\")\n",
    "print(f\"Best LSH:          {best['runtime_sec']:.1f}s → {int(best['candidate_pairs']):,} pairs (tier-2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 LSH + NPI Complementarity Analysis\n",
    "\n",
    "The real value of LSH is finding matches that exact NPI **cannot** — tier-2 records\n",
    "without NPIs. Here we quantify the additive value of LSH on top of NPI matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4.6 LSH + NPI COMPLEMENTARITY\n",
      "============================================================\n",
      "Tier-2 OP records total:  4,683\n",
      "Strategy B OP coverage:   4,574 (97.7%)\n",
      "\n",
      "--- Combined Pipeline Coverage ---\n",
      "Total OP records:         969,703\n",
      "  Tier-1 NPI matched:     542,329 (55.9%)\n",
      "  Tier-2 B-covered:       4,574 (0.5%)\n",
      "  Unmatchable:            31,405 (3.2%)\n",
      "  Overall linkable:       546,903 (56.4%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4.6 LSH + NPI COMPLEMENTARITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Tier-1: exact NPI covers 933,615 OP records → matched to Medicare via NPI\n",
    "# Tier-2: 12,813 OP records with NO NPI → must use fuzzy/LSH\n",
    "# Unmatchable: 31,424 → excluded\n",
    "\n",
    "# Of the tier-2 records, how many does each strategy cover?\n",
    "# OP coverage = unique OP indices in candidate pairs\n",
    "\n",
    "# Strategy B coverage\n",
    "op_covered_B = len(set(p[0] for p in set_B))\n",
    "\n",
    "# Best LSH coverage (we need to re-run at best config, or use sweep data)\n",
    "# For now, use the pairs count and estimate\n",
    "print(f\"Tier-2 OP records total:  {len(op_tier2):,}\")\n",
    "print(f\"Strategy B OP coverage:   {op_covered_B:,} ({op_covered_B/len(op_tier2)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- Combined Pipeline Coverage ---\")\n",
    "n_total_op = len(op_tier1) + len(op_tier2) + len(op_unmatchable)\n",
    "n_tier1_matched = len(npi_overlap)  # Approximate: unique NPIs overlapping\n",
    "n_tier2_covered_B = op_covered_B\n",
    "\n",
    "print(f\"Total OP records:         {n_total_op:,}\")\n",
    "print(f\"  Tier-1 NPI matched:     {n_tier1_matched:,} ({n_tier1_matched/n_total_op*100:.1f}%)\")\n",
    "print(f\"  Tier-2 B-covered:       {n_tier2_covered_B:,} ({n_tier2_covered_B/n_total_op*100:.1f}%)\")\n",
    "print(f\"  Unmatchable:            {len(op_unmatchable):,} ({len(op_unmatchable)/n_total_op*100:.1f}%)\")\n",
    "print(f\"  Overall linkable:       {n_tier1_matched + n_tier2_covered_B:,} \"\n",
    "      f\"({(n_tier1_matched + n_tier2_covered_B)/n_total_op*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Runtime Scaling Analysis\n",
    "\n",
    "Model how LSH runtime scales with dataset size and signature length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4.7 RUNTIME SCALING ANALYSIS\n",
      "============================================================\n",
      "num_perm=128: avg runtime 1695.8s across 1 thresholds\n",
      "\n",
      "--- Theoretical Scaling ---\n",
      "Current scale: 4,683 OP × 1,175,281 Med = 5,503,840,923 potential pairs\n",
      "Full-scale OP: 14,700,000 (scale factor: 3139x)\n",
      "Projected LSH runtime at full scale:\n",
      "  perm=128, thresh=0.5: 88719 min (1478.6 hr)\n",
      "\n",
      "For comparison:\n",
      "  Full cross-join at full scale: 17,276,630,700,000 pairs (infeasible)\n",
      "  Strategy B scales linearly with block sizes (minutes)\n",
      "  LSH scales ~O(n) per state with constant overhead for hashing\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4.7 RUNTIME SCALING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by num_perm to see scaling\n",
    "for np_val in NUM_PERMS:\n",
    "    subset = sweep_df[sweep_df[\"num_perm\"] == np_val]\n",
    "    avg_time = subset[\"runtime_sec\"].mean()\n",
    "    print(f\"num_perm={np_val:>3}: avg runtime {avg_time:.1f}s across {len(subset)} thresholds\")\n",
    "\n",
    "# Theoretical scaling\n",
    "print(f\"\\n--- Theoretical Scaling ---\")\n",
    "n_op = len(op_tier2)\n",
    "n_med = len(medclean)\n",
    "print(f\"Current scale: {n_op:,} OP × {n_med:,} Med = {n_op * n_med:,.0f} potential pairs\")\n",
    "\n",
    "# If we scaled to 14.7M OP records (full row-level):\n",
    "n_op_full = 14_700_000\n",
    "scale_factor = n_op_full / n_op\n",
    "print(f\"Full-scale OP: {n_op_full:,} (scale factor: {scale_factor:.0f}x)\")\n",
    "print(f\"Projected LSH runtime at full scale:\")\n",
    "for _, row in sweep_df.iterrows():\n",
    "    projected = row[\"runtime_sec\"] * scale_factor\n",
    "    print(f\"  perm={int(row['num_perm']):>3}, thresh={row['threshold']:.1f}: \"\n",
    "          f\"{projected/60:.0f} min ({projected/3600:.1f} hr)\")\n",
    "\n",
    "print(f\"\\nFor comparison:\")\n",
    "print(f\"  Full cross-join at full scale: {n_op_full * n_med:,.0f} pairs (infeasible)\")\n",
    "print(f\"  Strategy B scales linearly with block sizes (minutes)\")\n",
    "print(f\"  LSH scales ~O(n) per state with constant overhead for hashing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4.8 EXPORT\n",
      "============================================================\n",
      "✓ lsh_benchmark_results.csv (1 rows)\n",
      "✓ lsh_vs_baselines.csv (3 rows)\n",
      "\n",
      "All artifacts saved to ../artifacts/phase6_lsh_benchmark\n",
      "  lsh_benchmark_results.csv                     0.2 KB\n",
      "  lsh_vs_baselines.csv                          0.3 KB\n",
      "\n",
      "============================================================\n",
      "GAP 4 COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4.8 EXPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save sweep results\n",
    "sweep_df.to_csv(os.path.join(OUTPUTDIR, \"lsh_benchmark_results.csv\"), index=False)\n",
    "print(f\"✓ lsh_benchmark_results.csv ({len(sweep_df)} rows)\")\n",
    "\n",
    "# Save baselines comparison\n",
    "baselines = pd.DataFrame([\n",
    "    {\"method\": \"Exact NPI\", \"scope\": \"tier-1\", \"matches_or_pairs\": len(npi_overlap),\n",
    "     \"runtime_sec\": round(elapsed_npi, 2), \"reduction_ratio\": None,\n",
    "     \"note\": \"Gold standard for tier-1; cannot handle tier-2\"},\n",
    "    {\"method\": \"Strategy B (State+Soundex)\", \"scope\": \"tier-2\", \n",
    "     \"matches_or_pairs\": len(set_B), \"runtime_sec\": round(elapsed_B, 2),\n",
    "     \"reduction_ratio\": round(1 - len(set_B)/fullcross, 8),\n",
    "     \"note\": \"Best traditional blocking from Phase 3\"},\n",
    "    {\"method\": f\"LSH (perm={int(best['num_perm'])}, thresh={best['threshold']})\", \n",
    "     \"scope\": \"tier-2\",\n",
    "     \"matches_or_pairs\": int(best[\"candidate_pairs\"]),\n",
    "     \"runtime_sec\": best[\"runtime_sec\"],\n",
    "     \"reduction_ratio\": round(best[\"reduction_ratio\"], 8),\n",
    "     \"note\": \"Best LSH config from sweep\"},\n",
    "])\n",
    "baselines.to_csv(os.path.join(OUTPUTDIR, \"lsh_vs_baselines.csv\"), index=False)\n",
    "print(f\"✓ lsh_vs_baselines.csv ({len(baselines)} rows)\")\n",
    "\n",
    "# Cleanup\n",
    "op_tier2.drop(columns=\"_ns\", inplace=True, errors=\"ignore\")\n",
    "medclean.drop(columns=\"_ns\", inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(f\"\\nAll artifacts saved to {OUTPUTDIR}\")\n",
    "for f in sorted(os.listdir(OUTPUTDIR)):\n",
    "    sz = os.path.getsize(os.path.join(OUTPUTDIR, f)) / 1024\n",
    "    print(f\"  {f:40s} {sz:8.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GAP 4 COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
